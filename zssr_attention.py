import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import cv2
import tqdm
import random
import matplotlib.pyplot as plt
from collections import deque
from torch.cuda.amp import autocast, GradScaler
import os
import json
from datetime import datetime


# ====================================================================================
# üéØ –¶–ï–ù–¢–†–ê–õ–¨–ù–ê–Ø –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø - –í–°–ï –ì–ò–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–´ –í –û–î–ù–û–ú –ú–ï–°–¢–ï
# ====================================================================================

class Config:
    """
    üéØ –í–°–ï –ì–ò–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–´ –ó–î–ï–°–¨!
    –ò–∑–º–µ–Ω–∏—Ç–µ –ª—é–±–æ–π –ø–∞—Ä–∞–º–µ—Ç—Ä - –æ—Å—Ç–∞–ª—å–Ω—ã–µ –æ—Å—Ç–∞–Ω—É—Ç—Å—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

    üìà –î–õ–Ø –£–õ–£–ß–®–ï–ù–ò–Ø –ö–ê–ß–ï–°–¢–í–ê (–º–µ–¥–ª–µ–Ω–Ω–µ–µ, –Ω–æ –ª—É—á—à–µ):
    - NUM_ITERATIONS = 30000-50000 (–±–æ–ª—å—à–µ –∏—Ç–µ—Ä–∞—Ü–∏–π)
    - CHANNELS = 128 (–±–æ–ª—å—à–µ –∫–∞–Ω–∞–ª–æ–≤)
    - NUM_BLOCKS = 12-16 (–±–æ–ª—å—à–µ –±–ª–æ–∫–æ–≤)
    - CROP_SIZE = 96-128 (–±–æ–ª—å—à–µ —Ä–∞–∑–º–µ—Ä –ø–∞—Ç—á–µ–π)
    - INITIAL_LR = 0.00005 (–º–µ–Ω—å—à–µ learning rate)
    - LOSS_EDGE_WEIGHT = 0.3 (–±–æ–ª—å—à–µ –≤–µ—Å –Ω–∞ –∫—Ä–∞—è)
    - LOSS_HF_WEIGHT = 0.2 (–±–æ–ª—å—à–µ –≤–µ—Å –Ω–∞ –¥–µ—Ç–∞–ª–∏)

    üöÄ –î–õ–Ø x8 –£–í–ï–õ–ò–ß–ï–ù–ò–Ø:
    - SR_FACTOR = 8
    - NUM_ITERATIONS = 30000-40000
    - CROP_SIZE = 48-64
    - NUM_BLOCKS = 12-16 (–±–æ–ª—å—à–µ –±–ª–æ–∫–æ–≤ –¥–ª—è —Å–ª–æ–∂–Ω–æ–π –∑–∞–¥–∞—á–∏)
    """

    # === –ü–£–¢–ò –ö –§–ê–ô–õ–ê–ú ===
    INPUT_FILE = "/home/vdidur/ZRRS_Attention/data/single_amsr2_image_2.npz"  # –ò–ó–ú–ï–ù–ò–¢–ï –ù–ê –°–í–û–ô –ü–£–¢–¨!
    OUTPUT_DIR = "/home/vdidur/ZRRS_Attention/results"  # –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

    # === –û–°–ù–û–í–ù–´–ï –ü–ê–†–ê–ú–ï–¢–†–´ ===
    SR_FACTOR = 8  # –§–∞–∫—Ç–æ—Ä —É–≤–µ–ª–∏—á–µ–Ω–∏—è: 4, 8 –∏–ª–∏ 16
    NUM_ITERATIONS = 30000  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π (None = –∞–≤—Ç–æ-–≤—ã–±–æ—Ä –ø–æ SR_FACTOR)
    CROP_SIZE = 256  # –†–∞–∑–º–µ—Ä –ø–∞—Ç—á–µ–π (None = –∞–≤—Ç–æ-–≤—ã–±–æ—Ä –ø–æ SR_FACTOR)

    # === –ê–†–•–ò–¢–ï–ö–¢–£–†–ê –°–ï–¢–ò ===
    CHANNELS = 64  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–∞–ª–æ–≤ –≤ —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ—è—Ö
    NUM_BLOCKS = 8  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ Residual Attention –±–ª–æ–∫–æ–≤
    ATTENTION_REDUCTION = 8  # –°—Ç–µ–ø–µ–Ω—å —Å–∂–∞—Ç–∏—è –≤ Channel Attention
    USE_SPATIAL_ATTENTION = True  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Spatial Attention

    # === –ü–ê–†–ê–ú–ï–¢–†–´ –û–ë–£–ß–ï–ù–ò–Ø ===
    INITIAL_LR = 0.00005  # –ù–∞—á–∞–ª—å–Ω—ã–π learning rate
    WEIGHT_DECAY = 1e-4  # L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
    ADAM_BETAS = (0.9, 0.999)  # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ Adam
    GRADIENT_CLIP = 1.0  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –≥—Ä–∞–¥–∏–µ–Ω—Ç

    # === –§–£–ù–ö–¶–ò–Ø –ü–û–¢–ï–†–¨ ===
    LOSS_L1_WEIGHT = 0.6  # –í–µ—Å L1 loss (–æ—Å–Ω–æ–≤–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å)
    LOSS_EDGE_WEIGHT = 0.15  # –í–µ—Å Edge loss (—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫—Ä–∞–µ–≤)
    LOSS_HF_WEIGHT = 0.25  # –í–µ—Å High-frequency loss (–¥–µ—Ç–∞–ª–∏)
    NORMALIZE_LOSS = True  # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å loss –¥–ª—è –ª—É—á—à–µ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞

    # === –ê–£–ì–ú–ï–ù–¢–ê–¶–ò–Ø –î–ê–ù–ù–´–• ===
    BLUR_MIN = 0.5  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Å–∏–≥–º–∞ —Ä–∞–∑–º—ã—Ç–∏—è –¥–ª—è –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏
    BLUR_MAX = 2.0  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–∏–≥–º–∞ —Ä–∞–∑–º—ã—Ç–∏—è
    NOISE_PROBABILITY = 0.8  # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —à—É–º–∞
    NOISE_MIN = 0.001  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å —à—É–º–∞
    NOISE_MAX = 0.01  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å —à—É–º–∞
    BRIGHTNESS_RANGE = (0.9, 1.1)  # –î–∏–∞–ø–∞–∑–æ–Ω –∏–∑–º–µ–Ω–µ–Ω–∏—è —è—Ä–∫–æ—Å—Ç–∏
    CONTRAST_RANGE = (0.9, 1.1)  # –î–∏–∞–ø–∞–∑–æ–Ω –∏–∑–º–µ–Ω–µ–Ω–∏—è –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∞

    # === –í–ê–õ–ò–î–ê–¶–ò–Ø –ò EARLY STOPPING ===
    VALIDATION_FREQ = 1000  # –ß–∞—Å—Ç–æ—Ç–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    PATIENCE = 10  # –¢–µ—Ä–ø–µ–Ω–∏–µ –¥–ª—è early stopping
    IMPROVEMENT_THRESHOLD = 0.998  # –ü–æ—Ä–æ–≥ —É–ª—É—á—à–µ–Ω–∏—è (99.5%)
    MAX_LOSS_THRESHOLD = 10.0  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –¥–æ–ø—É—Å—Ç–∏–º—ã–π loss

    # === INFERENCE ===
    USE_TTA = True  # Test Time Augmentation
    SHARPENING_STRENGTH = 0.5  # –°–∏–ª–∞ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–µ–∑–∫–æ—Å—Ç–∏

    # === –ù–û–í–´–ï –ü–ê–†–ê–ú–ï–¢–†–´ –î–õ–Ø INFERENCE ===
    # –ú–µ—Ç–æ–¥ –±–æ—Ä—å–±—ã —Å –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–∞–º–∏ —Å–µ—Ç–∫–∏
    USE_BLENDING = False  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–ª–µ–Ω–¥–∏–Ω–≥ (–∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π –º–µ—Ç–æ–¥ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º)
    USE_RANDOM_SHIFTS = True  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–ª—É—á–∞–π–Ω—ã–µ —Å–¥–≤–∏–≥–∏ (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥)
    # –ú–æ–∂–Ω–æ –≤–∫–ª—é—á–∏—Ç—å –æ–±–∞ –º–µ—Ç–æ–¥–∞ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –±–ª–µ–Ω–¥–∏–Ω–≥–∞
    INFERENCE_PATCH_SIZE = None  # None = –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä
    INFERENCE_OVERLAP = None  # None = –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä (–æ–±—ã—á–Ω–æ 25-30% –æ—Ç patch_size)

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–ª—É—á–∞–π–Ω—ã—Ö —Å–¥–≤–∏–≥–æ–≤
    NUM_SHIFT_PASSES = 6  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—Ö–æ–¥–æ–≤ —Å–æ —Å–ª—É—á–∞–π–Ω—ã–º–∏ —Å–¥–≤–∏–≥–∞–º–∏ (4-8)
    MAX_SHIFT_RATIO = 0.33  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Å–¥–≤–∏–≥ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Ä–∞–∑–º–µ—Ä–∞ –ø–∞—Ç—á–∞ (0.1-0.5)

    # === –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø –¢–ï–ú–ü–ï–†–ê–¢–£–†–´ ===
    TEMPERATURE_NORMALIZATION = "percentile"  # "percentile" –∏–ª–∏ "absolute"
    # –î–ª—è percentile —Ä–µ–∂–∏–º–∞:
    TEMP_PERCENTILE_LOW = 1  # –ù–∏–∂–Ω–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç–∏–ª—å (–æ–±—ã—á–Ω–æ 1-5)
    TEMP_PERCENTILE_HIGH = 99  # –í–µ—Ä—Ö–Ω–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç–∏–ª—å (–æ–±—ã—á–Ω–æ 95-99)
    # –î–ª—è absolute —Ä–µ–∂–∏–º–∞:
    TEMP_ABSOLUTE_MIN = 200.0  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≤ –ö–µ–ª—å–≤–∏–Ω–∞—Ö
    TEMP_ABSOLUTE_MAX = 320.0  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≤ –ö–µ–ª—å–≤–∏–Ω–∞—Ö

    # === –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –ü–ê–†–ê–ú–ï–¢–†–´ ===
    DEVICE = "auto"  # "auto", "cuda", "cpu" –∏–ª–∏ –Ω–æ–º–µ—Ä GPU (–Ω–∞–ø—Ä–∏–º–µ—Ä, "cuda:1")
    MIXED_PRECISION = True  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å mixed precision (FP16) - —Ç–æ–ª—å–∫–æ –¥–ª—è CUDA
    NUM_WORKERS = 0  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö
    SAVE_INTERMEDIATE = False  # –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    VERBOSE = True  # –ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥

    @classmethod
    def get_auto_params(cls):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ SR_FACTOR"""
        if cls.NUM_ITERATIONS is None:
            cls.NUM_ITERATIONS = {
                4: 15000,
                8: 20000,
                16: 25000
            }.get(cls.SR_FACTOR, 15000)

        if cls.CROP_SIZE is None:
            cls.CROP_SIZE = {
                4: 64,
                8: 48,
                16: 32
            }.get(cls.SR_FACTOR, 64)

        if cls.INFERENCE_PATCH_SIZE is None:
            cls.INFERENCE_PATCH_SIZE = {
                4: 128,
                8: 96,
                16: 64
            }.get(cls.SR_FACTOR, 128)

        if cls.INFERENCE_OVERLAP is None:
            # 25-30% –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ –ø–∞—Ç—á–∞
            cls.INFERENCE_OVERLAP = max(16, cls.INFERENCE_PATCH_SIZE // 4)

    @classmethod
    def to_dict(cls):
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –≤ —Å–ª–æ–≤–∞—Ä—å"""
        return {
            key: value for key, value in cls.__dict__.items()
            if not key.startswith('_') and not callable(value)
        }

    @classmethod
    def save(cls, path):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –≤ JSON"""
        with open(path, 'w') as f:
            json.dump(cls.to_dict(), f, indent=4)

    @classmethod
    def get_filename_suffix(cls):
        """–°–æ–∑–¥–∞–µ—Ç —Å—É—Ñ—Ñ–∏–∫—Å –¥–ª—è –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        # –î–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        defaults = {
            'SR_FACTOR': 4,
            'CHANNELS': 64,
            'NUM_BLOCKS': 8,
            'INITIAL_LR': 0.0001,
            'LOSS_L1_WEIGHT': 0.7,
            'LOSS_EDGE_WEIGHT': 0.2,
            'LOSS_HF_WEIGHT': 0.1
        }

        # –ù–∞—Ö–æ–¥–∏–º –∏–∑–º–µ–Ω–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        suffix_parts = []
        current_config = cls.to_dict()

        for key, default_value in defaults.items():
            if key in current_config and current_config[key] != default_value:
                # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
                value = current_config[key]
                if isinstance(value, float):
                    value_str = f"{value:.0e}" if value < 0.01 else f"{value:.3f}"
                else:
                    value_str = str(value)

                # –°–æ–∫—Ä–∞—â–∞–µ–º –∏–º—è –ø–∞—Ä–∞–º–µ—Ç—Ä–∞
                short_name = {
                    'SR_FACTOR': 'sr',
                    'CHANNELS': 'ch',
                    'NUM_BLOCKS': 'blocks',
                    'INITIAL_LR': 'lr',
                    'LOSS_L1_WEIGHT': 'l1w',
                    'LOSS_EDGE_WEIGHT': 'edgew',
                    'LOSS_HF_WEIGHT': 'hfw'
                }.get(key, key.lower())

                suffix_parts.append(f"{short_name}{value_str}")

        # –î–æ–±–∞–≤–ª—è–µ–º timestamp –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        suffix_parts.append(timestamp)

        return "_".join(suffix_parts)



# ==================== MAIN PROCESSING FUNCTION ====================
def process_satellite_data(npz_path):
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–ø—É—Ç–Ω–∏–∫–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""

    print(f"\nüöÄ Enhanced ZSSR v3 with Easy Configuration")
    print(f"üì° Processing: {npz_path}")
    print(f"üîç Target SR: {Config.SR_FACTOR}x")

    # Load data
    with np.load(npz_path, allow_pickle=True) as data:
        temperature = data['temperature'].astype(np.float32)
        metadata = data['metadata'].item() if hasattr(data['metadata'], 'item') else data['metadata']

    # Extract scale factor
    if isinstance(metadata, dict):
        scale_factor = metadata.get('scale_factor', 0.01)
    else:
        import re
        metadata_str = str(metadata)
        scale_match = re.search(r"'scale_factor': ([0-9.e-]+)", metadata_str)
        scale_factor = float(scale_match.group(1)) if scale_match else 0.01

    # Process temperature data
    temp_data = temperature * scale_factor

    # Handle missing values
    valid_mask = temp_data != 0
    if np.sum(valid_mask) > 0:
        median_temp = np.median(temp_data[valid_mask])
        temp_data[~valid_mask] = median_temp

    # Temperature normalization based on configuration
    if Config.TEMPERATURE_NORMALIZATION == "percentile":
        # Percentile-based normalization
        p_low = Config.TEMP_PERCENTILE_LOW
        p_high = Config.TEMP_PERCENTILE_HIGH
        temp_min, temp_max = np.percentile(temp_data[valid_mask], [p_low, p_high])
        print(f"üìä Using percentile normalization: {p_low}% - {p_high}%")
    else:  # absolute
        # Absolute temperature normalization
        temp_min = Config.TEMP_ABSOLUTE_MIN
        temp_max = Config.TEMP_ABSOLUTE_MAX
        print(f"üìä Using absolute normalization: {temp_min}K - {temp_max}K")

    # Clip and normalize
    temp_data = np.clip(temp_data, temp_min, temp_max)
    normalized_data = (temp_data - temp_min) / (temp_max - temp_min)

    print(f"üìä Data shape: {normalized_data.shape}")
    print(f"üå°Ô∏è Temperature range: {temp_min:.1f} - {temp_max:.1f} K")
    print(f"üå°Ô∏è Actual data range: {temp_data[valid_mask].min():.1f} - {temp_data[valid_mask].max():.1f} K")

    # Initialize model
    model = AttentionZSSRNet(input_channels=1)

    # Custom initialization
    def init_weights(m):
        if isinstance(m, nn.Conv2d):
            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.GroupNorm):
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)

    model.apply(init_weights)

    # Train model
    model = train_with_attention(model, normalized_data)

    # Apply model with TTA
    print("\nüñºÔ∏è Generating super-resolved output...")
    enhanced_normalized = inference_with_tta_new(model, normalized_data)

    # Denormalize
    enhanced_temp = enhanced_normalized * (temp_max - temp_min) + temp_min

    # Save results with temperature range info
    print(f"\nüìä Temperature statistics:")
    print(f"   Original: min={temp_data.min():.2f} K, max={temp_data.max():.2f} K")
    print(f"   Enhanced: min={enhanced_temp.min():.2f} K, max={enhanced_temp.max():.2f} K")

    save_results_with_comparison(normalized_data, enhanced_normalized, temp_min, temp_max)

    # Generate filename with config suffix
    suffix = Config.get_filename_suffix()

    # Save model
    model_path = os.path.join(Config.OUTPUT_DIR, f'attention_zssr_{suffix}.pth')
    torch.save({
        'model_state_dict': model.state_dict(),
        'temperature_range': (temp_min, temp_max),
        'normalization_type': Config.TEMPERATURE_NORMALIZATION
    }, model_path)

    # Save config
    config_path = os.path.join(Config.OUTPUT_DIR, f'config_{suffix}.json')
    config_dict = Config.to_dict()
    with open(config_path, 'w') as f:
        json.dump(config_dict, f, indent=4)

    print(f"\nüíæ Model saved: {model_path}")
    print(f"üìÑ Config saved: {config_path}")

    return enhanced_temp, model  # Enhanced ZSSR v3 with Configurable Inference Methods


# ====================================================================================
# –ü–†–ò–ú–ï–ù–Ø–ï–ú –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Æ
# ====================================================================================

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
Config.get_auto_params()


# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
def get_device():
    if Config.DEVICE == "auto":
        return torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    elif Config.DEVICE.startswith("cuda") and torch.cuda.is_available():
        return torch.device(Config.DEVICE)
    else:
        return torch.device('cpu')


# ====================================================================================
# –û–°–¢–ê–õ–¨–ù–û–ô –ö–û–î –° –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï–ú Config
# ====================================================================================

# ==================== CHANNEL ATTENTION MODULE ====================
class ChannelAttention(nn.Module):
    """–õ–µ–≥–∫–æ–≤–µ—Å–Ω—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –ø–æ –∫–∞–Ω–∞–ª–∞–º –¥–ª—è —É—Å–∏–ª–µ–Ω–∏—è –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""

    def __init__(self, channels):
        super(ChannelAttention, self).__init__()
        reduction = Config.ATTENTION_REDUCTION
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)

        # Shared MLP
        self.fc1 = nn.Conv2d(channels, channels // reduction, 1, bias=False)
        self.relu = nn.ReLU(inplace=True)
        self.fc2 = nn.Conv2d(channels // reduction, channels, 1, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        # Average pooling path
        avg_out = self.fc2(self.relu(self.fc1(self.avg_pool(x))))
        # Max pooling path
        max_out = self.fc2(self.relu(self.fc1(self.max_pool(x))))
        # Combine and apply sigmoid
        attention = self.sigmoid(avg_out + max_out)
        return x * attention


# ==================== SPATIAL ATTENTION MODULE ====================
class SpatialAttention(nn.Module):
    """–ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∫–∏ –Ω–∞ –≤–∞–∂–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö"""

    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()
        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size // 2, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        # Channel-wise statistics
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        # Concatenate and convolve
        concat = torch.cat([avg_out, max_out], dim=1)
        attention = self.sigmoid(self.conv(concat))
        return x * attention


# ==================== RESIDUAL ATTENTION BLOCK ====================
class ResidualAttentionBlock(nn.Module):
    """–û—Å—Ç–∞—Ç–æ—á–Ω—ã–π –±–ª–æ–∫ —Å –º–µ—Ö–∞–Ω–∏–∑–º–æ–º –≤–Ω–∏–º–∞–Ω–∏—è"""

    def __init__(self, channels, kernel_size=3, use_spatial=True):
        super(ResidualAttentionBlock, self).__init__()

        # Convolutions
        self.conv1 = nn.Conv2d(channels, channels, kernel_size, padding=kernel_size // 2, bias=True)
        self.conv2 = nn.Conv2d(channels, channels, kernel_size, padding=kernel_size // 2, bias=True)

        # GroupNorm instead of BatchNorm (works better with batch_size=1)
        self.norm1 = nn.GroupNorm(num_groups=8, num_channels=channels)
        self.norm2 = nn.GroupNorm(num_groups=8, num_channels=channels)

        # Attention modules
        self.channel_attention = ChannelAttention(channels)
        self.spatial_attention = SpatialAttention() if use_spatial and Config.USE_SPATIAL_ATTENTION else None

        # Activation
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        residual = x

        # First conv block
        out = self.conv1(x)
        out = self.norm1(out)
        out = self.relu(out)

        # Second conv block
        out = self.conv2(out)
        out = self.norm2(out)

        # Apply attention
        out = self.channel_attention(out)
        if self.spatial_attention is not None:
            out = self.spatial_attention(out)

        # Residual connection
        out = out + residual
        out = self.relu(out)

        return out


# ==================== ENHANCED ZSSR NETWORK WITH ATTENTION ====================
class AttentionZSSRNet(nn.Module):
    def __init__(self, input_channels=1):
        super(AttentionZSSRNet, self).__init__()

        channels = Config.CHANNELS
        num_blocks = Config.NUM_BLOCKS
        sr_factor = Config.SR_FACTOR

        self.sr_factor = sr_factor

        # Initial feature extraction with larger kernel for better context
        self.conv_first = nn.Conv2d(input_channels, channels, kernel_size=7, padding=3, bias=True)

        # Residual attention blocks
        self.res_blocks = nn.ModuleList()
        for i in range(num_blocks):
            # Use spatial attention every 2 blocks to reduce computation
            use_spatial = (i % 2 == 0)
            self.res_blocks.append(ResidualAttentionBlock(channels, use_spatial=use_spatial))

        # Feature fusion before upsampling
        self.fusion = nn.Conv2d(channels, channels, 3, padding=1, bias=True)

        # Sub-pixel convolution for upsampling (more efficient than deconv)
        self.upscale_layers = nn.ModuleList()
        num_upscale = int(np.log2(sr_factor))
        for _ in range(num_upscale):
            self.upscale_layers.append(
                nn.Sequential(
                    nn.Conv2d(channels, channels * 4, 3, padding=1),
                    nn.PixelShuffle(2),
                    nn.ReLU(inplace=True)
                )
            )

        # Output projection
        self.conv_last = nn.Conv2d(channels, input_channels, kernel_size=3, padding=1, bias=True)

        # Skip connection for residual learning
        self.skip_upsample = nn.Upsample(scale_factor=sr_factor, mode='bicubic', align_corners=False)

    def forward(self, x):
        # Bicubic upsampling for skip connection
        bicubic = self.skip_upsample(x)

        # Feature extraction
        feat = self.conv_first(x)
        feat = F.relu(feat, inplace=True)

        # Pass through residual attention blocks
        for block in self.res_blocks:
            feat = block(feat)

        # Feature fusion
        feat = self.fusion(feat)

        # Progressive upsampling
        for upscale in self.upscale_layers:
            feat = upscale(feat)

        # Final projection
        out = self.conv_last(feat)

        # Residual learning - predict the difference
        return out + bicubic


# ==================== EDGE-AWARE GRADIENT LOSS ====================
class EdgeAwareGradientLoss(nn.Module):
    """–ü–æ—Ç–µ—Ä—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ –∫—Ä–∞—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–µ–∑–∫–æ—Å—Ç–∏"""

    def __init__(self):
        super(EdgeAwareGradientLoss, self).__init__()

        # Sobel filters for edge detection
        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32).view(1, 1, 3, 3)
        sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32).view(1, 1, 3, 3)

        self.register_buffer('sobel_x', sobel_x)
        self.register_buffer('sobel_y', sobel_y)

    def forward(self, output, target):
        # FIXED: Move sobel filters to the same device and dtype as input
        sobel_x = self.sobel_x.to(device=output.device, dtype=output.dtype)
        sobel_y = self.sobel_y.to(device=output.device, dtype=output.dtype)

        # Apply Sobel filters
        output_grad_x = F.conv2d(output, sobel_x, padding=1)
        output_grad_y = F.conv2d(output, sobel_y, padding=1)
        target_grad_x = F.conv2d(target, sobel_x, padding=1)
        target_grad_y = F.conv2d(target, sobel_y, padding=1)

        # Compute gradient magnitude
        output_grad_mag = torch.sqrt(output_grad_x ** 2 + output_grad_y ** 2 + 1e-8)
        target_grad_mag = torch.sqrt(target_grad_x ** 2 + target_grad_y ** 2 + 1e-8)

        # L1 loss on gradient magnitudes
        return F.l1_loss(output_grad_mag, target_grad_mag)


# ==================== SHARPNESS-ENHANCED LOSS ====================
class SharpnessEnhancedLoss(nn.Module):
    """–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ —Ä–µ–∑–∫–æ—Å—Ç—å"""

    def __init__(self):
        super(SharpnessEnhancedLoss, self).__init__()
        self.alpha = Config.LOSS_L1_WEIGHT
        self.beta = Config.LOSS_EDGE_WEIGHT
        self.gamma = Config.LOSS_HF_WEIGHT

        self.l1_loss = nn.L1Loss()
        self.edge_loss = EdgeAwareGradientLoss()

    def high_frequency_loss(self, output, target):
        """–ü–æ—Ç–µ—Ä—è –Ω–∞ –≤—ã—Å–æ–∫–∏—Ö —á–∞—Å—Ç–æ—Ç–∞—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–µ—Ç–∞–ª–µ–π"""
        # Simple high-pass filter
        kernel = torch.tensor([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]],
                              dtype=torch.float32).view(1, 1, 3, 3)

        # FIXED: Move kernel to the same device and dtype as input
        kernel = kernel.to(device=output.device, dtype=output.dtype)

        output_hf = F.conv2d(output, kernel, padding=1)
        target_hf = F.conv2d(target, kernel, padding=1)

        return F.l1_loss(output_hf, target_hf)

    def forward(self, output, target):
        l1 = self.l1_loss(output, target)
        edge = self.edge_loss(output, target)
        hf = self.high_frequency_loss(output, target)

        # Normalize losses if configured
        if Config.NORMALIZE_LOSS:
            # Normalize each component to similar scale
            l1_norm = l1
            edge_norm = edge * 0.5  # Edge loss tends to be ~2x larger
            hf_norm = hf * 0.3  # HF loss tends to be ~3x larger

            total_loss = self.alpha * l1_norm + self.beta * edge_norm + self.gamma * hf_norm
        else:
            total_loss = self.alpha * l1 + self.beta * edge + self.gamma * hf

        return total_loss, {'l1': l1.item(), 'edge': edge.item(), 'hf': hf.item()}


# ==================== DIRECT ARRAY DATA SAMPLER ====================
class DirectArrayDataSampler:
    """–†–∞–±–æ—Ç–∞–µ—Ç –Ω–∞–ø—Ä—è–º—É—é —Å numpy –º–∞—Å—Å–∏–≤–∞–º–∏ –±–µ–∑ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""

    def __init__(self, data_array):
        self.data = data_array.astype(np.float32)
        self.sr_factor = Config.SR_FACTOR
        self.crop_size = Config.CROP_SIZE

        # Pre-compute multi-scale versions
        self.scales = self._create_multiscale_data()

    def _create_multiscale_data(self):
        """–°–æ–∑–¥–∞–µ—Ç –≤–µ—Ä—Å–∏–∏ –¥–∞–Ω–Ω—ã—Ö –≤ —Ä–∞–∑–Ω—ã—Ö –º–∞—Å—à—Ç–∞–±–∞—Ö"""
        h, w = self.data.shape
        scales = []

        # Create different scales
        scale_factors = np.linspace(0.5, 1.0, 8)

        for scale in scale_factors:
            if scale == 1.0:
                scales.append(self.data)
            else:
                new_h, new_w = int(h * scale), int(w * scale)
                # Anti-aliasing before downsampling
                sigma = 1.0 / scale
                blurred = cv2.GaussianBlur(self.data, (0, 0), sigma)
                scaled = cv2.resize(blurred, (new_w, new_h), interpolation=cv2.INTER_LINEAR)
                scales.append(scaled)

        return scales

    def _apply_degradation(self, hr_patch):
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—É—é –¥–µ–≥—Ä–∞–¥–∞—Ü–∏—é –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è LR –ø–∞—Ç—á–∞"""
        # Random blur strength
        blur_sigma = np.random.uniform(Config.BLUR_MIN, Config.BLUR_MAX)

        # Apply Gaussian blur
        blurred = cv2.GaussianBlur(hr_patch, (0, 0), blur_sigma)

        # Downscale
        h, w = hr_patch.shape
        lr_h, lr_w = h // self.sr_factor, w // self.sr_factor
        lr = cv2.resize(blurred, (lr_w, lr_h), interpolation=cv2.INTER_LINEAR)

        # Add noise
        if np.random.random() < Config.NOISE_PROBABILITY:
            noise_sigma = np.random.uniform(Config.NOISE_MIN, Config.NOISE_MAX)
            noise = np.random.normal(0, noise_sigma, lr.shape).astype(np.float32)
            lr = lr + noise

        # Upscale back
        lr_upscaled = cv2.resize(lr, (w, h), interpolation=cv2.INTER_CUBIC)

        return np.clip(lr_upscaled, 0, 1)

    def get_training_pair(self):
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–∞—Ä—É HR-LR –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        # Random scale selection
        scale_idx = np.random.randint(len(self.scales))
        data = self.scales[scale_idx]

        h, w = data.shape

        # –í–ê–ñ–ù–û: –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ–º HR –ø–∞—Ç—á–∏ —Ä–∞–∑–º–µ—Ä–æ–º crop_size * sr_factor
        # —á—Ç–æ–±—ã –ø–æ—Å–ª–µ –¥–∞—É–Ω—Å—ç–º–ø–ª–∏–Ω–≥–∞ –ø–æ–ª—É—á–∏—Ç—å LR –ø–∞—Ç—á–∏ —Ä–∞–∑–º–µ—Ä–æ–º crop_size
        hr_crop_size = self.crop_size * self.sr_factor

        # Ensure we can crop
        if h < hr_crop_size or w < hr_crop_size:
            # Pad if necessary
            pad_h = max(0, hr_crop_size - h)
            pad_w = max(0, hr_crop_size - w)
            data = np.pad(data, ((0, pad_h), (0, pad_w)), mode='reflect')
            h, w = data.shape

        # Random crop with bias towards high-variance regions
        if np.random.random() < 0.7:  # 70% chance to select informative region
            # Compute local variance
            var_map = cv2.Laplacian(data, cv2.CV_32F)
            var_map = cv2.GaussianBlur(np.abs(var_map), (11, 11), 0)

            # Find high variance region
            valid_h = h - hr_crop_size
            valid_w = w - hr_crop_size

            if valid_h > 0 and valid_w > 0:
                # Sample from top 20% variance regions
                var_thresh = np.percentile(var_map[:valid_h, :valid_w], 80)
                high_var_mask = var_map[:valid_h, :valid_w] > var_thresh
                high_var_coords = np.argwhere(high_var_mask)

                if len(high_var_coords) > 0:
                    idx = np.random.randint(len(high_var_coords))
                    y, x = high_var_coords[idx]
                else:
                    y = np.random.randint(0, valid_h + 1)
                    x = np.random.randint(0, valid_w + 1)
            else:
                y, x = 0, 0
        else:
            # Random crop
            y = np.random.randint(0, h - hr_crop_size + 1)
            x = np.random.randint(0, w - hr_crop_size + 1)

        # Extract HR patch (—Ä–∞–∑–º–µ—Ä: hr_crop_size √ó hr_crop_size)
        hr_patch = data[y:y + hr_crop_size, x:x + hr_crop_size].copy()

        # Random augmentations
        if np.random.random() < 0.5:
            hr_patch = np.fliplr(hr_patch).copy()
        if np.random.random() < 0.5:
            hr_patch = np.flipud(hr_patch).copy()
        if np.random.random() < 0.5:
            k = np.random.randint(1, 4)
            hr_patch = np.rot90(hr_patch, k).copy()

        # Brightness/contrast augmentation
        if np.random.random() < 0.3:
            brightness = np.random.uniform(*Config.BRIGHTNESS_RANGE)
            hr_patch = np.clip(hr_patch * brightness, 0, 1)

        if np.random.random() < 0.3:
            contrast = np.random.uniform(*Config.CONTRAST_RANGE)
            mean = np.mean(hr_patch)
            hr_patch = np.clip((hr_patch - mean) * contrast + mean, 0, 1)

        # Create LR patch by downsampling HR patch
        # LR –±—É–¥–µ—Ç —Ä–∞–∑–º–µ—Ä–æ–º crop_size √ó crop_size
        lr_patch = cv2.resize(hr_patch, (self.crop_size, self.crop_size),
                              interpolation=cv2.INTER_LINEAR)

        # Apply degradation to LR
        lr_patch = self._apply_degradation_to_lr(lr_patch)

        # Convert to tensors
        hr_tensor = torch.from_numpy(hr_patch).unsqueeze(0).unsqueeze(0).float()
        lr_tensor = torch.from_numpy(lr_patch).unsqueeze(0).unsqueeze(0).float()

        return hr_tensor, lr_tensor

    def _apply_degradation_to_lr(self, lr_patch):
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç –¥–µ–≥—Ä–∞–¥–∞—Ü–∏—é –∫ LR –ø–∞—Ç—á—É"""
        # Random blur
        if np.random.random() < 0.7:
            blur_sigma = np.random.uniform(Config.BLUR_MIN, Config.BLUR_MAX)
            lr_patch = cv2.GaussianBlur(lr_patch, (0, 0), blur_sigma)

        # Add noise
        if np.random.random() < Config.NOISE_PROBABILITY:
            noise_sigma = np.random.uniform(Config.NOISE_MIN, Config.NOISE_MAX)
            noise = np.random.normal(0, noise_sigma, lr_patch.shape).astype(np.float32)
            lr_patch = lr_patch + noise

        return np.clip(lr_patch, 0, 1)


# ==================== TRAINING WITH LOSS MONITORING ====================
def train_with_attention(model, data_array):
    """–û–±—É—á–µ–Ω–∏–µ —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º –ø–æ—Ç–µ—Ä—å –∏ early stopping"""

    device = get_device()
    model = model.to(device)

    if Config.VERBOSE:
        print(f"\nüéØ Training AttentionZSSR Model")
        print(f"Device: {device}")
        print(f"SR Factor: {Config.SR_FACTOR}x")
        print(f"Initial LR: {Config.INITIAL_LR}")
        print(f"Iterations: {Config.NUM_ITERATIONS}")

    # Loss function
    criterion = SharpnessEnhancedLoss()

    # Optimizer with weight decay for regularization
    optimizer = optim.AdamW(model.parameters(), lr=Config.INITIAL_LR,
                            betas=Config.ADAM_BETAS, weight_decay=Config.WEIGHT_DECAY)

    # Scheduler with minimum LR
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=Config.NUM_ITERATIONS, eta_min=1e-6,
                                                     last_epoch=-1)

    # Mixed precision training for efficiency
    if Config.MIXED_PRECISION and device.type == 'cuda':
        try:
            # Try new PyTorch syntax first
            from torch.amp import GradScaler as NewGradScaler
            scaler = NewGradScaler('cuda')
            use_amp = True
            amp_style = 'new'
        except (ImportError, TypeError):
            # Fall back to old syntax
            from torch.cuda.amp import GradScaler
            scaler = GradScaler()
            use_amp = True
            amp_style = 'old'
    else:
        scaler = None
        use_amp = False
        amp_style = None

    # Data sampler
    sampler = DirectArrayDataSampler(data_array)

    # Loss tracking
    loss_history = deque(maxlen=100)
    loss_components = {'l1': deque(maxlen=100), 'edge': deque(maxlen=100), 'hf': deque(maxlen=100)}
    best_loss = float('inf')
    patience_counter = 0

    # Loss explosion protection
    loss_explosion_counter = 0

    with tqdm.tqdm(total=Config.NUM_ITERATIONS, disable=not Config.VERBOSE) as pbar:
        for batch_idx in range(Config.NUM_ITERATIONS):
            model.train()

            # Get training pair
            hr, lr = sampler.get_training_pair()
            hr, lr = hr.to(device), lr.to(device)

            # Forward pass
            if use_amp:
                if amp_style == 'new':
                    # New PyTorch syntax
                    from torch.amp import autocast
                    with autocast('cuda', dtype=torch.float16):
                        output = model(lr)
                        loss, components = criterion(output, hr)
                else:
                    # Old PyTorch syntax
                    from torch.cuda.amp import autocast
                    with autocast():
                        output = model(lr)
                        loss, components = criterion(output, hr)
            else:
                output = model(lr)
                loss, components = criterion(output, hr)

            # Check for loss explosion
            if loss.item() > Config.MAX_LOSS_THRESHOLD:
                loss_explosion_counter += 1
                if loss_explosion_counter > 3:
                    print(f"\n‚ö†Ô∏è Loss explosion detected! Stopping training.")
                    break
            else:
                loss_explosion_counter = 0

            # Backward pass
            optimizer.zero_grad()

            if use_amp and scaler is not None:
                scaler.scale(loss).backward()
                # Gradient clipping
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=Config.GRADIENT_CLIP)
                scaler.step(optimizer)
                scaler.update()
            else:
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=Config.GRADIENT_CLIP)
                optimizer.step()

            scheduler.step()

            # Track losses
            loss_val = loss.item()
            loss_history.append(loss_val)
            for key, value in components.items():
                loss_components[key].append(value)

            # Update progress bar
            current_lr = scheduler.get_last_lr()[0]
            pbar.set_description(
                f"Loss: {loss_val:.4f} | "
                f"L1: {components['l1']:.4f} | "
                f"Edge: {components['edge']:.4f} | "
                f"HF: {components['hf']:.4f} | "
                f"LR: {current_lr:.2e}"
            )
            pbar.update()

            # Validation and early stopping
            if batch_idx > 0 and batch_idx % Config.VALIDATION_FREQ == 0:
                avg_loss = np.mean(loss_history)

                if avg_loss < best_loss * Config.IMPROVEMENT_THRESHOLD:
                    best_loss = avg_loss
                    patience_counter = 0
                    best_model_state = model.state_dict().copy()

                    # Save intermediate checkpoint
                    if Config.SAVE_INTERMEDIATE:
                        checkpoint_path = os.path.join(Config.OUTPUT_DIR, f'checkpoint_iter{batch_idx}.pth')
                        torch.save({
                            'model_state_dict': best_model_state,
                            'iteration': batch_idx,
                            'loss': best_loss
                        }, checkpoint_path)
                else:
                    patience_counter += 1

                if patience_counter >= Config.PATIENCE:
                    print(f"\n‚úã Early stopping at iteration {batch_idx}")
                    # model.load_state_dict(best_model_state)
                    break

    return model