# Enhanced ZSSR v3 with Configurable Inference Methods
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import cv2
import tqdm
import random
import matplotlib.pyplot as plt
from collections import deque
from torch.cuda.amp import autocast, GradScaler
import os
import json
from datetime import datetime


# ====================================================================================
# üéØ –¶–ï–ù–¢–†–ê–õ–¨–ù–ê–Ø –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø - –í–°–ï –ì–ò–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–´ –í –û–î–ù–û–ú –ú–ï–°–¢–ï
# ====================================================================================

class Config:
    """
    üéØ –í–°–ï –ì–ò–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–´ –ó–î–ï–°–¨!
    –ò–∑–º–µ–Ω–∏—Ç–µ –ª—é–±–æ–π –ø–∞—Ä–∞–º–µ—Ç—Ä - –æ—Å—Ç–∞–ª—å–Ω—ã–µ –æ—Å—Ç–∞–Ω—É—Ç—Å—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

    üìà –î–õ–Ø –£–õ–£–ß–®–ï–ù–ò–Ø –ö–ê–ß–ï–°–¢–í–ê (–º–µ–¥–ª–µ–Ω–Ω–µ–µ, –Ω–æ –ª—É—á—à–µ):
    - NUM_ITERATIONS = 30000-50000 (–±–æ–ª—å—à–µ –∏—Ç–µ—Ä–∞—Ü–∏–π)
    - CHANNELS = 128 (–±–æ–ª—å—à–µ –∫–∞–Ω–∞–ª–æ–≤)
    - NUM_BLOCKS = 12-16 (–±–æ–ª—å—à–µ –±–ª–æ–∫–æ–≤)
    - CROP_SIZE = 96-128 (–±–æ–ª—å—à–µ —Ä–∞–∑–º–µ—Ä –ø–∞—Ç—á–µ–π)
    - INITIAL_LR = 0.00005 (–º–µ–Ω—å—à–µ learning rate)
    - LOSS_EDGE_WEIGHT = 0.3 (–±–æ–ª—å—à–µ –≤–µ—Å –Ω–∞ –∫—Ä–∞—è)
    - LOSS_HF_WEIGHT = 0.2 (–±–æ–ª—å—à–µ –≤–µ—Å –Ω–∞ –¥–µ—Ç–∞–ª–∏)

    üöÄ –î–õ–Ø x8 –£–í–ï–õ–ò–ß–ï–ù–ò–Ø:
    - SR_FACTOR = 8
    - NUM_ITERATIONS = 30000-40000
    - CROP_SIZE = 48-64
    - NUM_BLOCKS = 12-16 (–±–æ–ª—å—à–µ –±–ª–æ–∫–æ–≤ –¥–ª—è —Å–ª–æ–∂–Ω–æ–π –∑–∞–¥–∞—á–∏)
    """

    # === –ü–£–¢–ò –ö –§–ê–ô–õ–ê–ú ===
    INPUT_FILE = "/home/vdidur/ZRRS_Attention/data/single_amsr2_image_2.npz"  # –ò–ó–ú–ï–ù–ò–¢–ï –ù–ê –°–í–û–ô –ü–£–¢–¨!
    OUTPUT_DIR = "/home/vdidur/ZRRS_Attention/results"  # –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

    # === –û–°–ù–û–í–ù–´–ï –ü–ê–†–ê–ú–ï–¢–†–´ ===
    SR_FACTOR = 8  # –§–∞–∫—Ç–æ—Ä —É–≤–µ–ª–∏—á–µ–Ω–∏—è: 4, 8 –∏–ª–∏ 16
    NUM_ITERATIONS = 40000  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π (None = –∞–≤—Ç–æ-–≤—ã–±–æ—Ä –ø–æ SR_FACTOR)
    CROP_SIZE = 256  # –†–∞–∑–º–µ—Ä –ø–∞—Ç—á–µ–π (None = –∞–≤—Ç–æ-–≤—ã–±–æ—Ä –ø–æ SR_FACTOR)

    # === –ê–†–•–ò–¢–ï–ö–¢–£–†–ê –°–ï–¢–ò ===
    CHANNELS = 64  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–∞–ª–æ–≤ –≤ —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ—è—Ö
    NUM_BLOCKS = 12  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ Residual Attention –±–ª–æ–∫–æ–≤
    ATTENTION_REDUCTION = 8  # –°—Ç–µ–ø–µ–Ω—å —Å–∂–∞—Ç–∏—è –≤ Channel Attention
    USE_SPATIAL_ATTENTION = True  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Spatial Attention

    # === –ü–ê–†–ê–ú–ï–¢–†–´ –û–ë–£–ß–ï–ù–ò–Ø ===
    INITIAL_LR = 0.00005  # –ù–∞—á–∞–ª—å–Ω—ã–π learning rate
    WEIGHT_DECAY = 1e-4  # L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
    ADAM_BETAS = (0.9, 0.999)  # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ Adam
    GRADIENT_CLIP = 1.0  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –≥—Ä–∞–¥–∏–µ–Ω—Ç

    # === –§–£–ù–ö–¶–ò–Ø –ü–û–¢–ï–†–¨ ===
    LOSS_L1_WEIGHT = 0.6  # –í–µ—Å L1 loss (–æ—Å–Ω–æ–≤–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å)
    LOSS_EDGE_WEIGHT = 0.15  # –í–µ—Å Edge loss (—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫—Ä–∞–µ–≤)
    LOSS_HF_WEIGHT = 0.25  # –í–µ—Å High-frequency loss (–¥–µ—Ç–∞–ª–∏)
    NORMALIZE_LOSS = True  # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å loss –¥–ª—è –ª—É—á—à–µ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞

    # === –ê–£–ì–ú–ï–ù–¢–ê–¶–ò–Ø –î–ê–ù–ù–´–• ===
    BLUR_MIN = 0.5  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Å–∏–≥–º–∞ —Ä–∞–∑–º—ã—Ç–∏—è –¥–ª—è –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏
    BLUR_MAX = 2.0  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–∏–≥–º–∞ —Ä–∞–∑–º—ã—Ç–∏—è
    NOISE_PROBABILITY = 0.8  # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —à—É–º–∞
    NOISE_MIN = 0.001  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å —à—É–º–∞
    NOISE_MAX = 0.01  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å —à—É–º–∞
    BRIGHTNESS_RANGE = (0.9, 1.1)  # –î–∏–∞–ø–∞–∑–æ–Ω –∏–∑–º–µ–Ω–µ–Ω–∏—è —è—Ä–∫–æ—Å—Ç–∏
    CONTRAST_RANGE = (0.9, 1.1)  # –î–∏–∞–ø–∞–∑–æ–Ω –∏–∑–º–µ–Ω–µ–Ω–∏—è –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∞

    # === –í–ê–õ–ò–î–ê–¶–ò–Ø –ò EARLY STOPPING ===
    VALIDATION_FREQ = 10000  # –ß–∞—Å—Ç–æ—Ç–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    PATIENCE = 10  # –¢–µ—Ä–ø–µ–Ω–∏–µ –¥–ª—è early stopping
    IMPROVEMENT_THRESHOLD = 0.998  # –ü–æ—Ä–æ–≥ —É–ª—É—á—à–µ–Ω–∏—è (99.5%)
    MAX_LOSS_THRESHOLD = 10.0  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –¥–æ–ø—É—Å—Ç–∏–º—ã–π loss

    # === INFERENCE ===
    USE_TTA = True  # Test Time Augmentation
    SHARPENING_STRENGTH = 0.5  # –°–∏–ª–∞ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–µ–∑–∫–æ—Å—Ç–∏

    # === –ù–û–í–´–ï –ü–ê–†–ê–ú–ï–¢–†–´ –î–õ–Ø INFERENCE ===
    # –ú–µ—Ç–æ–¥ –±–æ—Ä—å–±—ã —Å –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–∞–º–∏ —Å–µ—Ç–∫–∏
    USE_BLENDING = False  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–ª–µ–Ω–¥–∏–Ω–≥ (–∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π –º–µ—Ç–æ–¥ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º)
    USE_RANDOM_SHIFTS = True  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–ª—É—á–∞–π–Ω—ã–µ —Å–¥–≤–∏–≥–∏ (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥)
    # –ú–æ–∂–Ω–æ –≤–∫–ª—é—á–∏—Ç—å –æ–±–∞ –º–µ—Ç–æ–¥–∞ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –±–ª–µ–Ω–¥–∏–Ω–≥–∞
    INFERENCE_PATCH_SIZE = None  # None = –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä
    INFERENCE_OVERLAP = None  # None = –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä (–æ–±—ã—á–Ω–æ 25-30% –æ—Ç patch_size)

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–ª—É—á–∞–π–Ω—ã—Ö —Å–¥–≤–∏–≥–æ–≤
    NUM_SHIFT_PASSES = 6  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—Ö–æ–¥–æ–≤ —Å–æ —Å–ª—É—á–∞–π–Ω—ã–º–∏ —Å–¥–≤–∏–≥–∞–º–∏ (4-8)
    MAX_SHIFT_RATIO = 0.33  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Å–¥–≤–∏–≥ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Ä–∞–∑–º–µ—Ä–∞ –ø–∞—Ç—á–∞ (0.1-0.5)

    # === –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø –¢–ï–ú–ü–ï–†–ê–¢–£–†–´ ===
    TEMPERATURE_NORMALIZATION = "percentile"  # "percentile" –∏–ª–∏ "absolute"
    # –î–ª—è percentile —Ä–µ–∂–∏–º–∞:
    TEMP_PERCENTILE_LOW = 1  # –ù–∏–∂–Ω–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç–∏–ª—å (–æ–±—ã—á–Ω–æ 1-5)
    TEMP_PERCENTILE_HIGH = 99  # –í–µ—Ä—Ö–Ω–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç–∏–ª—å (–æ–±—ã—á–Ω–æ 95-99)
    # –î–ª—è absolute —Ä–µ–∂–∏–º–∞:
    TEMP_ABSOLUTE_MIN = 200.0  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≤ –ö–µ–ª—å–≤–∏–Ω–∞—Ö
    TEMP_ABSOLUTE_MAX = 320.0  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≤ –ö–µ–ª—å–≤–∏–Ω–∞—Ö

    # === –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –ü–ê–†–ê–ú–ï–¢–†–´ ===
    DEVICE = "auto"  # "auto", "cuda", "cpu" –∏–ª–∏ –Ω–æ–º–µ—Ä GPU (–Ω–∞–ø—Ä–∏–º–µ—Ä, "cuda:1")
    MIXED_PRECISION = True  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å mixed precision (FP16) - —Ç–æ–ª—å–∫–æ –¥–ª—è CUDA
    NUM_WORKERS = 0  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö
    SAVE_INTERMEDIATE = False  # –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    VERBOSE = True  # –ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥

    @classmethod
    def get_auto_params(cls):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ SR_FACTOR"""
        if cls.NUM_ITERATIONS is None:
            cls.NUM_ITERATIONS = {
                4: 15000,
                8: 20000,
                16: 25000
            }.get(cls.SR_FACTOR, 15000)

        if cls.CROP_SIZE is None:
            cls.CROP_SIZE = {
                4: 64,
                8: 48,
                16: 32
            }.get(cls.SR_FACTOR, 64)

        if cls.INFERENCE_PATCH_SIZE is None:
            cls.INFERENCE_PATCH_SIZE = {
                4: 128,
                8: 96,
                16: 64
            }.get(cls.SR_FACTOR, 128)

        if cls.INFERENCE_OVERLAP is None:
            # 25-30% –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ –ø–∞—Ç—á–∞
            cls.INFERENCE_OVERLAP = max(16, cls.INFERENCE_PATCH_SIZE // 4)

    @classmethod
    def to_dict(cls):
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –≤ —Å–ª–æ–≤–∞—Ä—å"""
        return {
            key: value for key, value in cls.__dict__.items()
            if not key.startswith('_') and not callable(value)
        }

    @classmethod
    def save(cls, path):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –≤ JSON"""
        with open(path, 'w') as f:
            json.dump(cls.to_dict(), f, indent=4)

    @classmethod
    def get_filename_suffix(cls):
        """–°–æ–∑–¥–∞–µ—Ç —Å—É—Ñ—Ñ–∏–∫—Å –¥–ª—è –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        # –î–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        defaults = {
            'SR_FACTOR': 4,
            'CHANNELS': 64,
            'NUM_BLOCKS': 8,
            'INITIAL_LR': 0.0001,
            'LOSS_L1_WEIGHT': 0.7,
            'LOSS_EDGE_WEIGHT': 0.2,
            'LOSS_HF_WEIGHT': 0.1
        }

        # –ù–∞—Ö–æ–¥–∏–º –∏–∑–º–µ–Ω–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        suffix_parts = []
        current_config = cls.to_dict()

        for key, default_value in defaults.items():
            if key in current_config and current_config[key] != default_value:
                # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
                value = current_config[key]
                if isinstance(value, float):
                    value_str = f"{value:.0e}" if value < 0.01 else f"{value:.3f}"
                else:
                    value_str = str(value)

                # –°–æ–∫—Ä–∞—â–∞–µ–º –∏–º—è –ø–∞—Ä–∞–º–µ—Ç—Ä–∞
                short_name = {
                    'SR_FACTOR': 'sr',
                    'CHANNELS': 'ch',
                    'NUM_BLOCKS': 'blocks',
                    'INITIAL_LR': 'lr',
                    'LOSS_L1_WEIGHT': 'l1w',
                    'LOSS_EDGE_WEIGHT': 'edgew',
                    'LOSS_HF_WEIGHT': 'hfw'
                }.get(key, key.lower())

                suffix_parts.append(f"{short_name}{value_str}")

        # –î–æ–±–∞–≤–ª—è–µ–º timestamp –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        suffix_parts.append(timestamp)

        return "_".join(suffix_parts)


# ====================================================================================
# –ü–†–ò–ú–ï–ù–Ø–ï–ú –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Æ
# ====================================================================================

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
Config.get_auto_params()


# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
def get_device():
    if Config.DEVICE == "auto":
        return torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    elif Config.DEVICE.startswith("cuda") and torch.cuda.is_available():
        return torch.device(Config.DEVICE)
    else:
        return torch.device('cpu')


# ====================================================================================
# –û–°–¢–ê–õ–¨–ù–û–ô –ö–û–î –° –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï–ú Config
# ====================================================================================

# ==================== CHANNEL ATTENTION MODULE ====================
class ChannelAttention(nn.Module):
    """–õ–µ–≥–∫–æ–≤–µ—Å–Ω—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –ø–æ –∫–∞–Ω–∞–ª–∞–º –¥–ª—è —É—Å–∏–ª–µ–Ω–∏—è –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""

    def __init__(self, channels):
        super(ChannelAttention, self).__init__()
        reduction = Config.ATTENTION_REDUCTION
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)

        # Shared MLP
        self.fc1 = nn.Conv2d(channels, channels // reduction, 1, bias=False)
        self.relu = nn.ReLU(inplace=True)
        self.fc2 = nn.Conv2d(channels // reduction, channels, 1, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        # Average pooling path
        avg_out = self.fc2(self.relu(self.fc1(self.avg_pool(x))))
        # Max pooling path
        max_out = self.fc2(self.relu(self.fc1(self.max_pool(x))))
        # Combine and apply sigmoid
        attention = self.sigmoid(avg_out + max_out)
        return x * attention


# ==================== SPATIAL ATTENTION MODULE ====================
class SpatialAttention(nn.Module):
    """–ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∫–∏ –Ω–∞ –≤–∞–∂–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö"""

    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()
        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size // 2, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        # Channel-wise statistics
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        # Concatenate and convolve
        concat = torch.cat([avg_out, max_out], dim=1)
        attention = self.sigmoid(self.conv(concat))
        return x * attention


# ==================== RESIDUAL ATTENTION BLOCK ====================
class ResidualAttentionBlock(nn.Module):
    """–û—Å—Ç–∞—Ç–æ—á–Ω—ã–π –±–ª–æ–∫ —Å –º–µ—Ö–∞–Ω–∏–∑–º–æ–º –≤–Ω–∏–º–∞–Ω–∏—è"""

    def __init__(self, channels, kernel_size=3, use_spatial=True):
        super(ResidualAttentionBlock, self).__init__()

        # Convolutions
        self.conv1 = nn.Conv2d(channels, channels, kernel_size, padding=kernel_size // 2, bias=True)
        self.conv2 = nn.Conv2d(channels, channels, kernel_size, padding=kernel_size // 2, bias=True)

        # GroupNorm instead of BatchNorm (works better with batch_size=1)
        self.norm1 = nn.GroupNorm(num_groups=8, num_channels=channels)
        self.norm2 = nn.GroupNorm(num_groups=8, num_channels=channels)

        # Attention modules
        self.channel_attention = ChannelAttention(channels)
        self.spatial_attention = SpatialAttention() if use_spatial and Config.USE_SPATIAL_ATTENTION else None

        # Activation
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        residual = x

        # First conv block
        out = self.conv1(x)
        out = self.norm1(out)
        out = self.relu(out)

        # Second conv block
        out = self.conv2(out)
        out = self.norm2(out)

        # Apply attention
        out = self.channel_attention(out)
        if self.spatial_attention is not None:
            out = self.spatial_attention(out)

        # Residual connection
        out = out + residual
        out = self.relu(out)

        return out


# ==================== ENHANCED ZSSR NETWORK WITH ATTENTION ====================
class AttentionZSSRNet(nn.Module):
    def __init__(self, input_channels=1):
        super(AttentionZSSRNet, self).__init__()

        channels = Config.CHANNELS
        num_blocks = Config.NUM_BLOCKS
        sr_factor = Config.SR_FACTOR

        self.sr_factor = sr_factor

        # Initial feature extraction with larger kernel for better context
        self.conv_first = nn.Conv2d(input_channels, channels, kernel_size=7, padding=3, bias=True)

        # Residual attention blocks
        self.res_blocks = nn.ModuleList()
        for i in range(num_blocks):
            # Use spatial attention every 2 blocks to reduce computation
            use_spatial = (i % 2 == 0)
            self.res_blocks.append(ResidualAttentionBlock(channels, use_spatial=use_spatial))

        # Feature fusion before upsampling
        self.fusion = nn.Conv2d(channels, channels, 3, padding=1, bias=True)

        # Sub-pixel convolution for upsampling (more efficient than deconv)
        self.upscale_layers = nn.ModuleList()
        num_upscale = int(np.log2(sr_factor))
        for _ in range(num_upscale):
            self.upscale_layers.append(
                nn.Sequential(
                    nn.Conv2d(channels, channels * 4, 3, padding=1),
                    nn.PixelShuffle(2),
                    nn.ReLU(inplace=True)
                )
            )

        # Output projection
        self.conv_last = nn.Conv2d(channels, input_channels, kernel_size=3, padding=1, bias=True)

        # Skip connection for residual learning
        self.skip_upsample = nn.Upsample(scale_factor=sr_factor, mode='bicubic', align_corners=False)

    def forward(self, x):
        # Bicubic upsampling for skip connection
        bicubic = self.skip_upsample(x)

        # Feature extraction
        feat = self.conv_first(x)
        feat = F.relu(feat, inplace=True)

        # Pass through residual attention blocks
        for block in self.res_blocks:
            feat = block(feat)

        # Feature fusion
        feat = self.fusion(feat)

        # Progressive upsampling
        for upscale in self.upscale_layers:
            feat = upscale(feat)

        # Final projection
        out = self.conv_last(feat)

        # Residual learning - predict the difference
        return out + bicubic


# ==================== EDGE-AWARE GRADIENT LOSS ====================
class EdgeAwareGradientLoss(nn.Module):
    """–ü–æ—Ç–µ—Ä—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ –∫—Ä–∞—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–µ–∑–∫–æ—Å—Ç–∏"""

    def __init__(self):
        super(EdgeAwareGradientLoss, self).__init__()

        # Sobel filters for edge detection
        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32).view(1, 1, 3, 3)
        sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32).view(1, 1, 3, 3)

        self.register_buffer('sobel_x', sobel_x)
        self.register_buffer('sobel_y', sobel_y)

    def forward(self, output, target):
        # FIXED: Move sobel filters to the same device and dtype as input
        sobel_x = self.sobel_x.to(device=output.device, dtype=output.dtype)
        sobel_y = self.sobel_y.to(device=output.device, dtype=output.dtype)

        # Apply Sobel filters
        output_grad_x = F.conv2d(output, sobel_x, padding=1)
        output_grad_y = F.conv2d(output, sobel_y, padding=1)
        target_grad_x = F.conv2d(target, sobel_x, padding=1)
        target_grad_y = F.conv2d(target, sobel_y, padding=1)

        # Compute gradient magnitude
        output_grad_mag = torch.sqrt(output_grad_x ** 2 + output_grad_y ** 2 + 1e-8)
        target_grad_mag = torch.sqrt(target_grad_x ** 2 + target_grad_y ** 2 + 1e-8)

        # L1 loss on gradient magnitudes
        return F.l1_loss(output_grad_mag, target_grad_mag)


# ==================== SHARPNESS-ENHANCED LOSS ====================
class SharpnessEnhancedLoss(nn.Module):
    """–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ —Ä–µ–∑–∫–æ—Å—Ç—å"""

    def __init__(self):
        super(SharpnessEnhancedLoss, self).__init__()
        self.alpha = Config.LOSS_L1_WEIGHT
        self.beta = Config.LOSS_EDGE_WEIGHT
        self.gamma = Config.LOSS_HF_WEIGHT

        self.l1_loss = nn.L1Loss()
        self.edge_loss = EdgeAwareGradientLoss()

    def high_frequency_loss(self, output, target):
        """–ü–æ—Ç–µ—Ä—è –Ω–∞ –≤—ã—Å–æ–∫–∏—Ö —á–∞—Å—Ç–æ—Ç–∞—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–µ—Ç–∞–ª–µ–π"""
        # Simple high-pass filter
        kernel = torch.tensor([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]],
                              dtype=torch.float32).view(1, 1, 3, 3)

        # FIXED: Move kernel to the same device and dtype as input
        kernel = kernel.to(device=output.device, dtype=output.dtype)

        output_hf = F.conv2d(output, kernel, padding=1)
        target_hf = F.conv2d(target, kernel, padding=1)

        return F.l1_loss(output_hf, target_hf)

    def forward(self, output, target):
        l1 = self.l1_loss(output, target)
        edge = self.edge_loss(output, target)
        hf = self.high_frequency_loss(output, target)

        # Normalize losses if configured
        if Config.NORMALIZE_LOSS:
            # Normalize each component to similar scale
            l1_norm = l1
            edge_norm = edge * 0.5  # Edge loss tends to be ~2x larger
            hf_norm = hf * 0.3  # HF loss tends to be ~3x larger

            total_loss = self.alpha * l1_norm + self.beta * edge_norm + self.gamma * hf_norm
        else:
            total_loss = self.alpha * l1 + self.beta * edge + self.gamma * hf

        return total_loss, {'l1': l1.item(), 'edge': edge.item(), 'hf': hf.item()}


# ==================== DIRECT ARRAY DATA SAMPLER ====================
class DirectArrayDataSampler:
    """–†–∞–±–æ—Ç–∞–µ—Ç –Ω–∞–ø—Ä—è–º—É—é —Å numpy –º–∞—Å—Å–∏–≤–∞–º–∏ –±–µ–∑ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""

    def __init__(self, data_array):
        self.data = data_array.astype(np.float32)
        self.sr_factor = Config.SR_FACTOR
        self.crop_size = Config.CROP_SIZE

        # Pre-compute multi-scale versions
        self.scales = self._create_multiscale_data()

    def _create_multiscale_data(self):
        """–°–æ–∑–¥–∞–µ—Ç –≤–µ—Ä—Å–∏–∏ –¥–∞–Ω–Ω—ã—Ö –≤ —Ä–∞–∑–Ω—ã—Ö –º–∞—Å—à—Ç–∞–±–∞—Ö"""
        h, w = self.data.shape
        scales = []

        # Create different scales
        scale_factors = np.linspace(0.5, 1.0, 8)

        for scale in scale_factors:
            if scale == 1.0:
                scales.append(self.data)
            else:
                new_h, new_w = int(h * scale), int(w * scale)
                # Anti-aliasing before downsampling
                sigma = 1.0 / scale
                blurred = cv2.GaussianBlur(self.data, (0, 0), sigma)
                scaled = cv2.resize(blurred, (new_w, new_h), interpolation=cv2.INTER_LINEAR)
                scales.append(scaled)

        return scales

    def _apply_degradation(self, hr_patch):
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—É—é –¥–µ–≥—Ä–∞–¥–∞—Ü–∏—é –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è LR –ø–∞—Ç—á–∞"""
        # Random blur strength
        blur_sigma = np.random.uniform(Config.BLUR_MIN, Config.BLUR_MAX)

        # Apply Gaussian blur
        blurred = cv2.GaussianBlur(hr_patch, (0, 0), blur_sigma)

        # Downscale
        h, w = hr_patch.shape
        lr_h, lr_w = h // self.sr_factor, w // self.sr_factor
        lr = cv2.resize(blurred, (lr_w, lr_h), interpolation=cv2.INTER_LINEAR)

        # Add noise
        if np.random.random() < Config.NOISE_PROBABILITY:
            noise_sigma = np.random.uniform(Config.NOISE_MIN, Config.NOISE_MAX)
            noise = np.random.normal(0, noise_sigma, lr.shape).astype(np.float32)
            lr = lr + noise

        # Upscale back
        lr_upscaled = cv2.resize(lr, (w, h), interpolation=cv2.INTER_CUBIC)

        return np.clip(lr_upscaled, 0, 1)

    def get_training_pair(self):
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–∞—Ä—É HR-LR –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        # Random scale selection
        scale_idx = np.random.randint(len(self.scales))
        data = self.scales[scale_idx]

        h, w = data.shape

        # –í–ê–ñ–ù–û: –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ–º HR –ø–∞—Ç—á–∏ —Ä–∞–∑–º–µ—Ä–æ–º crop_size * sr_factor
        # —á—Ç–æ–±—ã –ø–æ—Å–ª–µ –¥–∞—É–Ω—Å—ç–º–ø–ª–∏–Ω–≥–∞ –ø–æ–ª—É—á–∏—Ç—å LR –ø–∞—Ç—á–∏ —Ä–∞–∑–º–µ—Ä–æ–º crop_size
        hr_crop_size = self.crop_size * self.sr_factor

        # Ensure we can crop
        if h < hr_crop_size or w < hr_crop_size:
            # Pad if necessary
            pad_h = max(0, hr_crop_size - h)
            pad_w = max(0, hr_crop_size - w)
            data = np.pad(data, ((0, pad_h), (0, pad_w)), mode='reflect')
            h, w = data.shape

        # Random crop with bias towards high-variance regions
        if np.random.random() < 0.7:  # 70% chance to select informative region
            # Compute local variance
            var_map = cv2.Laplacian(data, cv2.CV_32F)
            var_map = cv2.GaussianBlur(np.abs(var_map), (11, 11), 0)

            # Find high variance region
            valid_h = h - hr_crop_size
            valid_w = w - hr_crop_size

            if valid_h > 0 and valid_w > 0:
                # Sample from top 20% variance regions
                var_thresh = np.percentile(var_map[:valid_h, :valid_w], 80)
                high_var_mask = var_map[:valid_h, :valid_w] > var_thresh
                high_var_coords = np.argwhere(high_var_mask)

                if len(high_var_coords) > 0:
                    idx = np.random.randint(len(high_var_coords))
                    y, x = high_var_coords[idx]
                else:
                    y = np.random.randint(0, valid_h + 1)
                    x = np.random.randint(0, valid_w + 1)
            else:
                y, x = 0, 0
        else:
            # Random crop
            y = np.random.randint(0, h - hr_crop_size + 1)
            x = np.random.randint(0, w - hr_crop_size + 1)

        # Extract HR patch (—Ä–∞–∑–º–µ—Ä: hr_crop_size √ó hr_crop_size)
        hr_patch = data[y:y + hr_crop_size, x:x + hr_crop_size].copy()

        # Random augmentations
        if np.random.random() < 0.5:
            hr_patch = np.fliplr(hr_patch).copy()
        if np.random.random() < 0.5:
            hr_patch = np.flipud(hr_patch).copy()
        if np.random.random() < 0.5:
            k = np.random.randint(1, 4)
            hr_patch = np.rot90(hr_patch, k).copy()

        # Brightness/contrast augmentation
        if np.random.random() < 0.3:
            brightness = np.random.uniform(*Config.BRIGHTNESS_RANGE)
            hr_patch = np.clip(hr_patch * brightness, 0, 1)

        if np.random.random() < 0.3:
            contrast = np.random.uniform(*Config.CONTRAST_RANGE)
            mean = np.mean(hr_patch)
            hr_patch = np.clip((hr_patch - mean) * contrast + mean, 0, 1)

        # Create LR patch by downsampling HR patch
        # LR –±—É–¥–µ—Ç —Ä–∞–∑–º–µ—Ä–æ–º crop_size √ó crop_size
        lr_patch = cv2.resize(hr_patch, (self.crop_size, self.crop_size),
                              interpolation=cv2.INTER_LINEAR)

        # Apply degradation to LR
        lr_patch = self._apply_degradation_to_lr(lr_patch)

        # Convert to tensors
        hr_tensor = torch.from_numpy(hr_patch).unsqueeze(0).unsqueeze(0).float()
        lr_tensor = torch.from_numpy(lr_patch).unsqueeze(0).unsqueeze(0).float()

        return hr_tensor, lr_tensor

    def _apply_degradation_to_lr(self, lr_patch):
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç –¥–µ–≥—Ä–∞–¥–∞—Ü–∏—é –∫ LR –ø–∞—Ç—á—É"""
        # Random blur
        if np.random.random() < 0.7:
            blur_sigma = np.random.uniform(Config.BLUR_MIN, Config.BLUR_MAX)
            lr_patch = cv2.GaussianBlur(lr_patch, (0, 0), blur_sigma)

        # Add noise
        if np.random.random() < Config.NOISE_PROBABILITY:
            noise_sigma = np.random.uniform(Config.NOISE_MIN, Config.NOISE_MAX)
            noise = np.random.normal(0, noise_sigma, lr_patch.shape).astype(np.float32)
            lr_patch = lr_patch + noise

        return np.clip(lr_patch, 0, 1)


# ==================== TRAINING WITH LOSS MONITORING ====================
def train_with_attention(model, data_array):
    """–û–±—É—á–µ–Ω–∏–µ —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º –ø–æ—Ç–µ—Ä—å –∏ early stopping"""

    device = get_device()
    model = model.to(device)

    if Config.VERBOSE:
        print(f"\nüéØ Training AttentionZSSR Model")
        print(f"Device: {device}")
        print(f"SR Factor: {Config.SR_FACTOR}x")
        print(f"Initial LR: {Config.INITIAL_LR}")
        print(f"Iterations: {Config.NUM_ITERATIONS}")

    # Loss function
    criterion = SharpnessEnhancedLoss()

    # Optimizer with weight decay for regularization
    optimizer = optim.AdamW(model.parameters(), lr=Config.INITIAL_LR,
                            betas=Config.ADAM_BETAS, weight_decay=Config.WEIGHT_DECAY)

    # Scheduler with minimum LR
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=Config.NUM_ITERATIONS, eta_min=1e-6,
                                                     last_epoch=-1)

    # Mixed precision training for efficiency
    if Config.MIXED_PRECISION and device.type == 'cuda':
        try:
            # Try new PyTorch syntax first
            from torch.amp import GradScaler as NewGradScaler
            scaler = NewGradScaler('cuda')
            use_amp = True
            amp_style = 'new'
        except (ImportError, TypeError):
            # Fall back to old syntax
            from torch.cuda.amp import GradScaler
            scaler = GradScaler()
            use_amp = True
            amp_style = 'old'
    else:
        scaler = None
        use_amp = False
        amp_style = None

    # Data sampler
    sampler = DirectArrayDataSampler(data_array)

    # Loss tracking
    loss_history = deque(maxlen=100)
    loss_components = {'l1': deque(maxlen=100), 'edge': deque(maxlen=100), 'hf': deque(maxlen=100)}
    best_loss = float('inf')
    patience_counter = 0

    # Loss explosion protection
    loss_explosion_counter = 0

    with tqdm.tqdm(total=Config.NUM_ITERATIONS, disable=not Config.VERBOSE) as pbar:
        for batch_idx in range(Config.NUM_ITERATIONS):
            model.train()

            # Get training pair
            hr, lr = sampler.get_training_pair()
            hr, lr = hr.to(device), lr.to(device)

            # Forward pass
            if use_amp:
                if amp_style == 'new':
                    # New PyTorch syntax
                    from torch.amp import autocast
                    with autocast('cuda', dtype=torch.float16):
                        output = model(lr)
                        loss, components = criterion(output, hr)
                else:
                    # Old PyTorch syntax
                    from torch.cuda.amp import autocast
                    with autocast():
                        output = model(lr)
                        loss, components = criterion(output, hr)
            else:
                output = model(lr)
                loss, components = criterion(output, hr)

            # Check for loss explosion
            if loss.item() > Config.MAX_LOSS_THRESHOLD:
                loss_explosion_counter += 1
                if loss_explosion_counter > 3:
                    print(f"\n‚ö†Ô∏è Loss explosion detected! Stopping training.")
                    break
            else:
                loss_explosion_counter = 0

            # Backward pass
            optimizer.zero_grad()

            if use_amp and scaler is not None:
                scaler.scale(loss).backward()
                # Gradient clipping
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=Config.GRADIENT_CLIP)
                scaler.step(optimizer)
                scaler.update()
            else:
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=Config.GRADIENT_CLIP)
                optimizer.step()

            scheduler.step()

            # Track losses
            loss_val = loss.item()
            loss_history.append(loss_val)
            for key, value in components.items():
                loss_components[key].append(value)

            # Update progress bar
            current_lr = scheduler.get_last_lr()[0]
            pbar.set_description(
                f"Loss: {loss_val:.4f} | "
                f"L1: {components['l1']:.4f} | "
                f"Edge: {components['edge']:.4f} | "
                f"HF: {components['hf']:.4f} | "
                f"LR: {current_lr:.2e}"
            )
            pbar.update()

            # Validation and early stopping
            if batch_idx > 0 and batch_idx % Config.VALIDATION_FREQ == 0:
                avg_loss = np.mean(loss_history)

                if avg_loss < best_loss * Config.IMPROVEMENT_THRESHOLD:
                    best_loss = avg_loss
                    patience_counter = 0
                    best_model_state = model.state_dict().copy()

                    # Save intermediate checkpoint
                    if Config.SAVE_INTERMEDIATE:
                        checkpoint_path = os.path.join(Config.OUTPUT_DIR, f'checkpoint_iter{batch_idx}.pth')
                        torch.save({
                            'model_state_dict': best_model_state,
                            'iteration': batch_idx,
                            'loss': best_loss
                        }, checkpoint_path)
                else:
                    patience_counter += 1

                if patience_counter >= Config.PATIENCE:
                    print(f"\n‚úã Early stopping at iteration {batch_idx}")
                    break

    return model


# ==================== HELPER FUNCTIONS ====================
def enhance_sharpness(image):
    """–ü–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–µ–∑–∫–æ—Å—Ç–∏"""
    # Unsharp masking
    blurred = cv2.GaussianBlur(image, (0, 0), 1.0)
    sharpened = image + Config.SHARPENING_STRENGTH * (image - blurred)

    # Ensure float32
    sharpened = sharpened.astype(np.float32)

    # Adaptive sharpening based on local variance
    local_var = cv2.Laplacian(image, cv2.CV_32F)
    local_var = cv2.GaussianBlur(np.abs(local_var), (5, 5), 0)

    # Normalize variance map
    var_min, var_max = local_var.min(), local_var.max()
    if var_max > var_min:
        var_norm = (local_var - var_min) / (var_max - var_min)
    else:
        var_norm = np.zeros_like(local_var)

    # Apply adaptive sharpening
    result = image * (1 - 0.3 * var_norm) + sharpened * (0.3 * var_norm)

    return np.clip(result, 0, 1)


def get_optimal_patch_params(image_shape, sr_factor):
    """
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è overlapping inference

    Args:
        image_shape: (h, w) —Ä–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        sr_factor: —Ñ–∞–∫—Ç–æ—Ä —É–≤–µ–ª–∏—á–µ–Ω–∏—è

    Returns:
        patch_size, overlap
    """
    h, w = image_shape

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ Config –µ—Å–ª–∏ –æ–Ω–∏ –∑–∞–¥–∞–Ω—ã
    if Config.INFERENCE_PATCH_SIZE is not None:
        patch_size = Config.INFERENCE_PATCH_SIZE
    else:
        # –ë–∞–∑–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä –ø–∞—Ç—á–∞ - –±–æ–ª—å—à–µ —á–µ–º –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        if sr_factor == 4:
            base_patch_size = 128
        elif sr_factor == 8:
            base_patch_size = 96
        else:  # sr_factor == 16
            base_patch_size = 64

        # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º –ø–æ–¥ —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        min_patches = 4  # –º–∏–Ω–∏–º—É–º –ø–∞—Ç—á–µ–π –Ω–∞ –∏–∑–º–µ—Ä–µ–Ω–∏–µ
        max_patch_size = min(h, w) // min_patches

        patch_size = min(base_patch_size, max_patch_size)

    if Config.INFERENCE_OVERLAP is not None:
        overlap = Config.INFERENCE_OVERLAP
    else:
        # Overlap - 25-30% –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ –ø–∞—Ç—á–∞
        overlap = max(16, patch_size // 4)

    # –£–±–µ–¥–∏–º—Å—è —á—Ç–æ patch_size –∫—Ä–∞—Ç–µ–Ω 8 –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    patch_size = (patch_size // 8) * 8
    overlap = (overlap // 8) * 8

    return patch_size, overlap


# ==================== INFERENCE WITH OVERLAPPING ====================
def inference_with_overlapping(model, data_array, patch_size=128, overlap=32):
    """
    Inference —Å –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—â–∏–º–∏—Å—è –ø–∞—Ç—á–∞–º–∏ –¥–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è —Å–µ—Ç–∫–∏

    Args:
        model: –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
        data_array: –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        patch_size: —Ä–∞–∑–º–µ—Ä –ø–∞—Ç—á–∞ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ (–±–æ–ª—å—à–µ —á–µ–º –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏)
        overlap: —Ä–∞–∑–º–µ—Ä –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –º–µ–∂–¥—É –ø–∞—Ç—á–∞–º–∏
    """
    device = get_device()
    model = model.to(device).eval()

    h, w = data_array.shape
    sr_factor = Config.SR_FACTOR

    # –í—ã—Ö–æ–¥–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã
    out_h, out_w = h * sr_factor, w * sr_factor

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ –º–∞—Å—Å–∏–≤–∞ –∏ –º–∞—Å—Å–∏–≤–∞ –≤–µ—Å–æ–≤
    output = np.zeros((out_h, out_w), dtype=np.float32)
    weights = np.zeros((out_h, out_w), dtype=np.float32)

    # –°–æ–∑–¥–∞–µ–º –≤–µ—Å–∞ –¥–ª—è –±–ª–µ–Ω–¥–∏–Ω–≥–∞ (Gaussian-like)
    def create_weight_mask(size, overlap):
        """–°–æ–∑–¥–∞–µ—Ç –º–∞—Å–∫—É –≤–µ—Å–æ–≤ —Å –ø–ª–∞–≤–Ω—ã–º –ø–µ—Ä–µ—Ö–æ–¥–æ–º –Ω–∞ –∫—Ä–∞—è—Ö"""
        mask = np.ones((size, size), dtype=np.float32)

        # –°–æ–∑–¥–∞–µ–º –ø–ª–∞–≤–Ω—ã–π –ø–µ—Ä–µ—Ö–æ–¥ –Ω–∞ –∫—Ä–∞—è—Ö
        fade_region = overlap // 2
        if fade_region > 0:
            # –õ–∏–Ω–µ–π–Ω—ã–π fade
            for i in range(fade_region):
                weight = (i + 1) / fade_region
                mask[i, :] *= weight
                mask[-i - 1, :] *= weight
                mask[:, i] *= weight
                mask[:, -i - 1] *= weight

        return mask

    # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∑–¥–∞–µ–º –º–∞—Å–∫—É –≤–µ—Å–æ–≤
    weight_mask = create_weight_mask(patch_size * sr_factor, overlap * sr_factor)

    # –í—ã—á–∏—Å–ª—è–µ–º —à–∞–≥ –º–µ–∂–¥—É –ø–∞—Ç—á–∞–º–∏
    stride = patch_size - overlap

    # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º
    with torch.no_grad():
        for y in range(0, h - overlap, stride):
            for x in range(0, w - overlap, stride):
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥—Ä–∞–Ω–∏—Ü—ã —Ç–µ–∫—É—â–µ–≥–æ –ø–∞—Ç—á–∞
                y_end = min(y + patch_size, h)
                x_end = min(x + patch_size, w)

                # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –ø–∞—Ç—á–µ–π
                if y_end == h and y_end - y < patch_size:
                    y = h - patch_size
                if x_end == w and x_end - x < patch_size:
                    x = w - patch_size

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ç—á
                patch = data_array[y:y + patch_size, x:x + patch_size]

                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ç–µ–Ω–∑–æ—Ä
                patch_tensor = torch.from_numpy(patch).float().unsqueeze(0).unsqueeze(0).to(device)

                # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–æ–¥–µ–ª—å
                sr_patch = model(patch_tensor)
                sr_patch = sr_patch.cpu().squeeze().numpy()

                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ–∑–∏—Ü–∏—é –≤ –≤—ã—Ö–æ–¥–Ω–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏
                out_y = y * sr_factor
                out_x = x * sr_factor
                out_y_end = min(out_y + patch_size * sr_factor, out_h)
                out_x_end = min(out_x + patch_size * sr_factor, out_w)

                # –†–∞–∑–º–µ—Ä—ã –≤—ã—Ö–æ–¥–Ω–æ–≥–æ –ø–∞—Ç—á–∞
                patch_h = out_y_end - out_y
                patch_w = out_x_end - out_x

                # –û–±—Ä–µ–∑–∞–µ–º –º–∞—Å–∫—É –≤–µ—Å–æ–≤ –µ—Å–ª–∏ –ø–∞—Ç—á –Ω–∞ –∫—Ä–∞—é
                current_weight_mask = weight_mask[:patch_h, :patch_w]

                # –î–æ–±–∞–≤–ª—è–µ–º –≤–∑–≤–µ—à–µ–Ω–Ω—ã–π –ø–∞—Ç—á –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
                output[out_y:out_y_end, out_x:out_x_end] += sr_patch[:patch_h, :patch_w] * current_weight_mask
                weights[out_y:out_y_end, out_x:out_x_end] += current_weight_mask

    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–æ –≤–µ—Å–∞–º
    output = np.divide(output, weights, out=output, where=weights > 0)

    # Post-processing –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è —à–≤–æ–≤
    if Config.SHARPENING_STRENGTH > 0:
        output = enhance_sharpness(output)

    return np.clip(output, 0, 1)


# ==================== INFERENCE WITH RANDOM SHIFTS ====================
def inference_with_random_shifts(model, data_array, patch_size, num_passes=None, max_shift_ratio=None):
    """
    Inference —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –ø—Ä–æ—Ö–æ–¥–∞–º–∏ –∏ —Å–ª—É—á–∞–π–Ω—ã–º–∏ —Å–¥–≤–∏–≥–∞–º–∏

    Args:
        model: –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
        data_array: –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        patch_size: —Ä–∞–∑–º–µ—Ä –ø–∞—Ç—á–∞ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
        num_passes: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—Ö–æ–¥–æ–≤ (–µ—Å–ª–∏ None, –±–µ—Ä–µ—Ç—Å—è –∏–∑ Config)
        max_shift_ratio: –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Å–¥–≤–∏–≥ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Ä–∞–∑–º–µ—Ä–∞ –ø–∞—Ç—á–∞ (–µ—Å–ª–∏ None, –±–µ—Ä–µ—Ç—Å—è –∏–∑ Config)
    """
    device = get_device()
    model = model.to(device).eval()

    if num_passes is None:
        num_passes = Config.NUM_SHIFT_PASSES
    if max_shift_ratio is None:
        max_shift_ratio = Config.MAX_SHIFT_RATIO

    h, w = data_array.shape
    sr_factor = Config.SR_FACTOR

    # –í—ã—Ö–æ–¥–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã
    out_h, out_w = h * sr_factor, w * sr_factor

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Å—Å–∏–≤–æ–≤ –¥–ª—è –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    accumulated_output = np.zeros((out_h, out_w), dtype=np.float32)
    count_map = np.zeros((out_h, out_w), dtype=np.float32)

    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Å–¥–≤–∏–≥ –≤ –ø–∏–∫—Å–µ–ª—è—Ö
    max_shift = int(patch_size * max_shift_ratio)

    print(f"\nüé≤ Random shifts inference:")
    print(f"   Number of passes: {num_passes}")
    print(f"   Max shift: {max_shift} pixels ({max_shift_ratio * 100:.0f}% of patch size)")

    with torch.no_grad():
        for pass_idx in range(num_passes):
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω—ã–π —Å–¥–≤–∏–≥ –¥–ª—è —ç—Ç–æ–≥–æ –ø—Ä–æ—Ö–æ–¥–∞
            shift_y = np.random.randint(-max_shift, max_shift + 1) if pass_idx > 0 else 0
            shift_x = np.random.randint(-max_shift, max_shift + 1) if pass_idx > 0 else 0

            # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ —Å–µ—Ç–∫–µ –ø–∞—Ç—á–µ–π –ë–ï–ó –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
            for y in range(0, h, patch_size):
                for x in range(0, w, patch_size):
                    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å–¥–≤–∏–≥
                    y_shifted = y + shift_y
                    x_shifted = x + shift_x

                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥—Ä–∞–Ω–∏—Ü—ã –ø–∞—Ç—á–∞ —Å —É—á–µ—Ç–æ–º –≥—Ä–∞–Ω–∏—Ü –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    y_start = max(0, y_shifted)
                    x_start = max(0, x_shifted)
                    y_end = min(h, y_shifted + patch_size)
                    x_end = min(w, x_shifted + patch_size)

                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ –ø–∞—Ç—á —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π (–º–µ–Ω—å—à–µ –ø–æ–ª–æ–≤–∏–Ω—ã)
                    if (y_end - y_start) < patch_size // 2 or (x_end - x_start) < patch_size // 2:
                        continue

                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ç—á
                    patch = data_array[y_start:y_end, x_start:x_end]

                    # –ü–∞–¥–¥–∏–Ω–≥ –¥–æ –Ω—É–∂–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –µ—Å–ª–∏ –ø–∞—Ç—á –æ–±—Ä–µ–∑–∞–Ω
                    if patch.shape[0] < patch_size or patch.shape[1] < patch_size:
                        pad_h = patch_size - patch.shape[0]
                        pad_w = patch_size - patch.shape[1]
                        patch = np.pad(patch, ((0, pad_h), (0, pad_w)), mode='reflect')

                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ç–µ–Ω–∑–æ—Ä –∏ –ø—Ä–∏–º–µ–Ω—è–µ–º –º–æ–¥–µ–ª—å
                    patch_tensor = torch.from_numpy(patch).float().unsqueeze(0).unsqueeze(0).to(device)
                    sr_patch = model(patch_tensor)
                    sr_patch = sr_patch.cpu().squeeze().numpy()

                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ–∑–∏—Ü–∏—é –≤ –≤—ã—Ö–æ–¥–Ω–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏
                    out_y_start = y_start * sr_factor
                    out_x_start = x_start * sr_factor
                    out_y_end = min(out_h, y_end * sr_factor)
                    out_x_end = min(out_w, x_end * sr_factor)

                    # –û–±—Ä–µ–∑–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –µ—Å–ª–∏ –æ–Ω –±—ã–ª –ø–∞–¥–¥–∏–Ω–≥–æ–≤–∞–Ω
                    actual_h = (y_end - y_start) * sr_factor
                    actual_w = (x_end - x_start) * sr_factor
                    sr_patch = sr_patch[:actual_h, :actual_w]

                    # –î–æ–±–∞–≤–ª—è–µ–º –∫ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω–æ–º—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
                    accumulated_output[out_y_start:out_y_end, out_x_start:out_x_end] += sr_patch
                    count_map[out_y_start:out_y_end, out_x_start:out_x_end] += 1

    # –£—Å—Ä–µ–¥–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    result = np.divide(accumulated_output, count_map, out=accumulated_output, where=count_map > 0)

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∏–∫—Å–µ–ª–µ–π –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –ø–æ–ø–∞–ª–∏ –Ω–∏ –≤ –æ–¥–∏–Ω –ø–∞—Ç—á (–µ—Å–ª–∏ –µ—Å—Ç—å)
    if np.any(count_map == 0):
        # –ü—Ä–æ—Å—Ç–∞—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –¥–ª—è –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –ø–∏–∫—Å–µ–ª–µ–π
        mask = count_map == 0
        if np.sum(mask) > 0:
            from scipy.ndimage import binary_dilation
            # –†–∞—Å—à–∏—Ä—è–µ–º –º–∞—Å–∫—É –¥–ª—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–∏
            dilated_mask = binary_dilation(mask, iterations=3)
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–ª–∏–∂–∞–π—à–∏–µ –≤–∞–ª–∏–¥–Ω—ã–µ –ø–∏–∫—Å–µ–ª–∏
            valid_mask = ~mask
            if np.any(valid_mask):
                from scipy.interpolate import griddata
                y_coords, x_coords = np.meshgrid(np.arange(out_h), np.arange(out_w), indexing='ij')
                valid_points = np.column_stack((y_coords[valid_mask], x_coords[valid_mask]))
                valid_values = result[valid_mask]
                invalid_points = np.column_stack((y_coords[mask], x_coords[mask]))
                if len(invalid_points) > 0 and len(valid_points) > 0:
                    interpolated = griddata(valid_points, valid_values, invalid_points, method='nearest')
                    result[mask] = interpolated

    return np.clip(result, 0, 1)


# ==================== INFERENCE WITH TTA ====================
def inference_with_tta(model, data_array):
    """Inference —Å Test Time Augmentation –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞"""
    device = get_device()
    model = model.to(device).eval()

    # Prepare input
    h, w = data_array.shape
    data_tensor = torch.from_numpy(data_array).float().unsqueeze(0).unsqueeze(0)

    if not Config.USE_TTA:
        # –ë–µ–∑ TTA
        with torch.no_grad():
            input_tensor = data_tensor.to(device)
            output = model(input_tensor)
            result = output.cpu().squeeze().numpy()
    else:
        # –° TTA
        augmented_outputs = []

        with torch.no_grad():
            # 1. Original
            input_tensor = data_tensor.to(device)
            output = model(input_tensor)
            augmented_outputs.append(output.cpu())

            # 2. Horizontal flip
            input_flipped = torch.flip(input_tensor, dims=[3])
            output_flipped = model(input_flipped)
            output_flipped = torch.flip(output_flipped, dims=[3])
            augmented_outputs.append(output_flipped.cpu())

            # 3. Vertical flip
            input_flipped = torch.flip(input_tensor, dims=[2])
            output_flipped = model(input_flipped)
            output_flipped = torch.flip(output_flipped, dims=[2])
            augmented_outputs.append(output_flipped.cpu())

            # 4. 90 degree rotations
            for k in [1, 2, 3]:
                input_rotated = torch.rot90(input_tensor, k, dims=[2, 3])
                output_rotated = model(input_rotated)
                output_rotated = torch.rot90(output_rotated, -k, dims=[2, 3])
                augmented_outputs.append(output_rotated.cpu())

        # Average all predictions
        final_output = torch.stack(augmented_outputs).mean(dim=0)
        result = final_output.squeeze().numpy()

    # Post-processing for sharpness
    result = enhance_sharpness(result)

    return np.clip(result, 0, 1)


# ==================== MAIN INFERENCE FUNCTION ====================
def inference_with_tta_new(model, data_array):
    """–ù–æ–≤–∞—è –≤–µ—Ä—Å–∏—è inference —Å –≤—ã–±–æ—Ä–æ–º –º–µ—Ç–æ–¥–∞"""
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    patch_size, overlap = get_optimal_patch_params(data_array.shape, Config.SR_FACTOR)

    device = get_device()
    model = model.to(device).eval()

    # –í—ã–±–∏—Ä–∞–µ–º –º–µ—Ç–æ–¥(—ã) inference
    methods_used = []
    if Config.USE_BLENDING:
        methods_used.append("blending")
    if Config.USE_RANDOM_SHIFTS:
        methods_used.append("random shifts")

    if not methods_used:
        # –ï—Å–ª–∏ –Ω–∏–∫–∞–∫–æ–π –º–µ—Ç–æ–¥ –Ω–µ –≤—ã–±—Ä–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π inference
        print("\n‚ö†Ô∏è No anti-aliasing method selected, using simple inference")
        return inference_with_tta(model, data_array)

    print(f"\nüîß Inference configuration:")
    print(f"   Methods: {', '.join(methods_used)}")
    print(f"   Patch size: {patch_size}x{patch_size}")
    if Config.USE_BLENDING:
        print(f"   Overlap: {overlap} pixels")
    if Config.USE_RANDOM_SHIFTS:
        print(f"   Random shift passes: {Config.NUM_SHIFT_PASSES}")
        print(f"   Max shift ratio: {Config.MAX_SHIFT_RATIO}")

    if not Config.USE_TTA:
        # –ë–µ–∑ TTA
        if Config.USE_BLENDING and Config.USE_RANDOM_SHIFTS:
            # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –æ–±–∞ –º–µ—Ç–æ–¥–∞
            result1 = inference_with_overlapping(model, data_array, patch_size, overlap)
            result2 = inference_with_random_shifts(model, data_array, patch_size)
            result = (result1 + result2) / 2
        elif Config.USE_BLENDING:
            # –¢–æ–ª—å–∫–æ –±–ª–µ–Ω–¥–∏–Ω–≥
            result = inference_with_overlapping(model, data_array, patch_size, overlap)
        else:  # Config.USE_RANDOM_SHIFTS
            # –¢–æ–ª—å–∫–æ —Å–ª—É—á–∞–π–Ω—ã–µ —Å–¥–≤–∏–≥–∏
            result = inference_with_random_shifts(model, data_array, patch_size)
    else:
        # –° TTA
        augmented_outputs = []

        # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞
        def apply_inference_method(data):
            if Config.USE_BLENDING and Config.USE_RANDOM_SHIFTS:
                result1 = inference_with_overlapping(model, data, patch_size, overlap)
                result2 = inference_with_random_shifts(model, data, patch_size)
                return (result1 + result2) / 2
            elif Config.USE_BLENDING:
                return inference_with_overlapping(model, data, patch_size, overlap)
            else:  # Config.USE_RANDOM_SHIFTS
                return inference_with_random_shifts(model, data, patch_size)

        # 1. Original
        output = apply_inference_method(data_array)
        augmented_outputs.append(output)

        # 2. Horizontal flip
        data_flipped = np.fliplr(data_array).copy()
        output_flipped = apply_inference_method(data_flipped)
        output_flipped = np.fliplr(output_flipped).copy()
        augmented_outputs.append(output_flipped)

        # 3. Vertical flip
        data_flipped = np.flipud(data_array).copy()
        output_flipped = apply_inference_method(data_flipped)
        output_flipped = np.flipud(output_flipped).copy()
        augmented_outputs.append(output_flipped)

        # 4. 90 degree rotations
        for k in [1, 2, 3]:
            data_rotated = np.rot90(data_array, k).copy()
            output_rotated = apply_inference_method(data_rotated)
            output_rotated = np.rot90(output_rotated, -k).copy()
            augmented_outputs.append(output_rotated)

        # Average all predictions
        result = np.mean(augmented_outputs, axis=0)

    return np.clip(result, 0, 1)


# ==================== SAVE RESULTS WITH COMPARISON ====================
def save_results_with_comparison(original_norm, enhanced_norm, temp_min, temp_max):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –≤–∏–∑—É–∞–ª—å–Ω—ã–º —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ–º"""

    # Denormalize for visualization
    original_temp = original_norm * (temp_max - temp_min) + temp_min
    enhanced_temp = enhanced_norm * (temp_max - temp_min) + temp_min

    print(f"\nüì∏ Saving results...")

    # Generate filename suffix
    suffix = Config.get_filename_suffix()

    # Calculate proper figure dimensions based on aspect ratio
    h, w = original_temp.shape
    ratio = w / h
    base_height = 12  # inches
    base_width = ratio * base_height

    # Create comparison figure with proper aspect ratio
    fig = plt.figure(figsize=(base_width * 2.2, base_height))

    # Original
    ax1 = plt.subplot(1, 2, 1)
    im1 = ax1.imshow(original_temp, cmap='turbo', aspect='auto')
    ax1.set_title(f'Original ({original_temp.shape[0]}√ó{original_temp.shape[1]})\n~10 km/pixel', fontsize=16)
    ax1.axis('off')
    cbar1 = plt.colorbar(im1, ax=ax1, fraction=0.046, pad=0.04)
    cbar1.ax.set_ylabel('Temperature (K)', fontsize=12)

    # Enhanced
    ax2 = plt.subplot(1, 2, 2)
    im2 = ax2.imshow(enhanced_temp, cmap='turbo', aspect='auto')
    ax2.set_title(
        f'Enhanced ({enhanced_temp.shape[0]}√ó{enhanced_temp.shape[1]})\n~{10 / Config.SR_FACTOR:.1f} km/pixel',
        fontsize=16)
    ax2.axis('off')
    cbar2 = plt.colorbar(im2, ax=ax2, fraction=0.046, pad=0.04)
    cbar2.ax.set_ylabel('Temperature (K)', fontsize=12)

    plt.tight_layout()
    plt.savefig(os.path.join(Config.OUTPUT_DIR, f'comparison_{suffix}.png'), dpi=300, bbox_inches='tight')
    plt.close()

    # Save individual images with proper aspect ratio
    # Original
    plt.figure(figsize=(base_width, base_height))
    plt.imshow(original_temp, cmap='turbo', aspect='auto')
    plt.axis('off')
    plt.savefig(os.path.join(Config.OUTPUT_DIR, f'original_{suffix}.png'), dpi=300, bbox_inches='tight', pad_inches=0)
    plt.close()

    # Enhanced
    enhanced_h, enhanced_w = enhanced_temp.shape
    enhanced_ratio = enhanced_w / enhanced_h
    enhanced_width = enhanced_ratio * base_height

    plt.figure(figsize=(enhanced_width, base_height))
    plt.imshow(enhanced_temp, cmap='turbo', aspect='auto')
    plt.axis('off')
    plt.savefig(os.path.join(Config.OUTPUT_DIR, f'enhanced_{suffix}.png'), dpi=300, bbox_inches='tight', pad_inches=0)
    plt.close()

    # Detail comparison - zoom on interesting region
    h, w = original_temp.shape
    crop_size = min(100, h // 4)

    # Find region with high variance (most interesting)
    var_map = cv2.Laplacian(original_norm.astype(np.float32), cv2.CV_32F)
    var_map = cv2.GaussianBlur(np.abs(var_map), (21, 21), 0)

    # Get coordinates of maximum variance
    max_var_idx = np.unravel_index(
        np.argmax(var_map[crop_size:-crop_size, crop_size:-crop_size]),
        var_map[crop_size:-crop_size, crop_size:-crop_size].shape
    )
    center_h = max_var_idx[0] + crop_size
    center_w = max_var_idx[1] + crop_size

    # Create detail comparison
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))

    # Original crop
    crop_orig = original_temp[center_h - crop_size // 2:center_h + crop_size // 2,
                center_w - crop_size // 2:center_w + crop_size // 2]
    axes[0].imshow(crop_orig, cmap='turbo', aspect='auto')
    axes[0].set_title('Original Detail', fontsize=14)
    axes[0].axis('off')

    # Bicubic upsampled crop (for comparison)
    crop_bicubic = cv2.resize(crop_orig, (crop_size * Config.SR_FACTOR, crop_size * Config.SR_FACTOR),
                              interpolation=cv2.INTER_CUBIC)
    axes[1].imshow(crop_bicubic, cmap='turbo', aspect='auto')
    axes[1].set_title('Bicubic Interpolation', fontsize=14)
    axes[1].axis('off')

    # Enhanced crop
    sr = Config.SR_FACTOR
    crop_enh = enhanced_temp[center_h * sr - crop_size * sr // 2:center_h * sr + crop_size * sr // 2,
               center_w * sr - crop_size * sr // 2:center_w * sr + crop_size * sr // 2]
    axes[2].imshow(crop_enh, cmap='turbo', aspect='auto')
    axes[2].set_title('AttentionZSSR (Ours)', fontsize=14)
    axes[2].axis('off')

    plt.tight_layout()
    plt.savefig(os.path.join(Config.OUTPUT_DIR, f'detail_{suffix}.png'), dpi=300, bbox_inches='tight')
    plt.close()

    # Save numpy arrays
    np.save(os.path.join(Config.OUTPUT_DIR, f'enhanced_data_{suffix}.npy'), enhanced_temp)
    np.save(os.path.join(Config.OUTPUT_DIR, f'original_data_{suffix}.npy'), original_temp)

    print("‚úÖ Results saved:")
    print(f"   üìÅ {Config.OUTPUT_DIR}/")
    print(f"      ‚îú‚îÄ‚îÄ comparison_{suffix}.png")
    print(f"      ‚îú‚îÄ‚îÄ original_{suffix}.png")
    print(f"      ‚îú‚îÄ‚îÄ enhanced_{suffix}.png")
    print(f"      ‚îú‚îÄ‚îÄ detail_{suffix}.png")
    print(f"      ‚îú‚îÄ‚îÄ enhanced_data_{suffix}.npy")
    print(f"      ‚îî‚îÄ‚îÄ original_data_{suffix}.npy")


# ==================== MAIN PROCESSING FUNCTION ====================
def process_satellite_data(npz_path):
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–ø—É—Ç–Ω–∏–∫–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""

    print(f"\nüöÄ Enhanced ZSSR v3 with Easy Configuration")
    print(f"üì° Processing: {npz_path}")
    print(f"üîç Target SR: {Config.SR_FACTOR}x")

    # Load data
    with np.load(npz_path, allow_pickle=True) as data:
        temperature = data['temperature'].astype(np.float32)
        metadata = data['metadata'].item() if hasattr(data['metadata'], 'item') else data['metadata']

    # Extract scale factor
    if isinstance(metadata, dict):
        scale_factor = metadata.get('scale_factor', 0.01)
    else:
        import re
        metadata_str = str(metadata)
        scale_match = re.search(r"'scale_factor': ([0-9.e-]+)", metadata_str)
        scale_factor = float(scale_match.group(1)) if scale_match else 0.01

    # Process temperature data
    temp_data = temperature * scale_factor

    # Handle missing values
    valid_mask = temp_data != 0
    if np.sum(valid_mask) > 0:
        median_temp = np.median(temp_data[valid_mask])
        temp_data[~valid_mask] = median_temp

    # Temperature normalization based on configuration
    if Config.TEMPERATURE_NORMALIZATION == "percentile":
        # Percentile-based normalization
        p_low = Config.TEMP_PERCENTILE_LOW
        p_high = Config.TEMP_PERCENTILE_HIGH
        temp_min, temp_max = np.percentile(temp_data[valid_mask], [p_low, p_high])
        print(f"üìä Using percentile normalization: {p_low}% - {p_high}%")
    else:  # absolute
        # Absolute temperature normalization
        temp_min = Config.TEMP_ABSOLUTE_MIN
        temp_max = Config.TEMP_ABSOLUTE_MAX
        print(f"üìä Using absolute normalization: {temp_min}K - {temp_max}K")

    # Clip and normalize
    temp_data = np.clip(temp_data, temp_min, temp_max)
    normalized_data = (temp_data - temp_min) / (temp_max - temp_min)

    print(f"üìä Data shape: {normalized_data.shape}")
    print(f"üå°Ô∏è Temperature range: {temp_min:.1f} - {temp_max:.1f} K")
    print(f"üå°Ô∏è Actual data range: {temp_data[valid_mask].min():.1f} - {temp_data[valid_mask].max():.1f} K")

    # Initialize model
    model = AttentionZSSRNet(input_channels=1)

    # Custom initialization
    def init_weights(m):
        if isinstance(m, nn.Conv2d):
            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.GroupNorm):
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)

    model.apply(init_weights)

    # Train model
    model = train_with_attention(model, normalized_data)

    # Apply model with TTA
    print("\nüñºÔ∏è Generating super-resolved output...")
    enhanced_normalized = inference_with_tta_new(model, normalized_data)

    # Denormalize
    enhanced_temp = enhanced_normalized * (temp_max - temp_min) + temp_min

    # Save results with temperature range info
    print(f"\nüìä Temperature statistics:")
    print(f"   Original: min={temp_data.min():.2f} K, max={temp_data.max():.2f} K")
    print(f"   Enhanced: min={enhanced_temp.min():.2f} K, max={enhanced_temp.max():.2f} K")

    save_results_with_comparison(normalized_data, enhanced_normalized, temp_min, temp_max)

    # Generate filename with config suffix
    suffix = Config.get_filename_suffix()

    # Save model
    model_path = os.path.join(Config.OUTPUT_DIR, f'attention_zssr_{suffix}.pth')
    torch.save({
        'model_state_dict': model.state_dict(),
        'temperature_range': (temp_min, temp_max),
        'normalization_type': Config.TEMPERATURE_NORMALIZATION
    }, model_path)

    # Save config
    config_path = os.path.join(Config.OUTPUT_DIR, f'config_{suffix}.json')
    config_dict = Config.to_dict()
    with open(config_path, 'w') as f:
        json.dump(config_dict, f, indent=4)

    print(f"\nüíæ Model saved: {model_path}")
    print(f"üìÑ Config saved: {config_path}")

    return enhanced_temp, model


# ==================== MAIN EXECUTION ====================
def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    # –°–æ–∑–¥–∞–µ–º –≤—ã—Ö–æ–¥–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    os.makedirs(Config.OUTPUT_DIR, exist_ok=True)

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
    if not os.path.exists(Config.INPUT_FILE):
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {Config.INPUT_FILE}")
        print("üìù –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∏–∑–º–µ–Ω–∏—Ç–µ Config.INPUT_FILE –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—É—Ç—å")
        return

    print(f"\n{'=' * 60}")
    print(f"üöÄ Enhanced ZSSR v3 - Easy Hyperparameter Configuration")
    print(f"{'=' * 60}")
    print(f"\nüìã Current Configuration:")
    print(f"   SR Factor: {Config.SR_FACTOR}x")
    print(f"   Iterations: {Config.NUM_ITERATIONS}")
    print(f"   Crop Size: {Config.CROP_SIZE}")
    print(f"   Channels: {Config.CHANNELS}")
    print(f"   Blocks: {Config.NUM_BLOCKS}")
    print(f"   Learning Rate: {Config.INITIAL_LR}")
    print(f"   Loss Weights: L1={Config.LOSS_L1_WEIGHT}, Edge={Config.LOSS_EDGE_WEIGHT}, HF={Config.LOSS_HF_WEIGHT}")
    print(f"\nüìä Temperature Normalization:")
    print(f"   Method: {Config.TEMPERATURE_NORMALIZATION}")
    if Config.TEMPERATURE_NORMALIZATION == "percentile":
        print(f"   Percentiles: {Config.TEMP_PERCENTILE_LOW}% - {Config.TEMP_PERCENTILE_HIGH}%")
    else:
        print(f"   Absolute range: {Config.TEMP_ABSOLUTE_MIN}K - {Config.TEMP_ABSOLUTE_MAX}K")
    print(f"\nüîß Inference Methods:")
    print(f"   Blending: {'‚úì' if Config.USE_BLENDING else '‚úó'}")
    print(f"   Random Shifts: {'‚úì' if Config.USE_RANDOM_SHIFTS else '‚úó'}")
    if Config.USE_RANDOM_SHIFTS:
        print(f"   - Passes: {Config.NUM_SHIFT_PASSES}")
        print(f"   - Max shift: {Config.MAX_SHIFT_RATIO * 100:.0f}%")
    print(f"   TTA: {'‚úì' if Config.USE_TTA else '‚úó'}")
    print(f"{'=' * 60}\n")

    try:
        # Process data
        enhanced_temp, model = process_satellite_data(Config.INPUT_FILE)

        print(f"\n‚úÖ Processing complete!")
        print(f"üìÅ All results saved in: {Config.OUTPUT_DIR}")

    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()